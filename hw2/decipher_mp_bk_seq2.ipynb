{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Decipherment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import defaultdict, Counter\n",
    "import collections\n",
    "import pprint\n",
    "import math\n",
    "import bz2\n",
    "from ngram import *\n",
    "import sys, string, os\n",
    "import copy\n",
    "import pickle\n",
    "#from joblib import Parallel, delayed\n",
    "import itertools\n",
    "from multiprocessing import Process,Pool, cpu_count\n",
    "import datetime, time, random\n",
    "pp = pprint.PrettyPrinter(width=45, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us read in the cipher text from the `data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    if filename[-4:] == \".bz2\":\n",
    "        with bz2.open(filename, 'rt', encoding='utf8') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    return content\n",
    "\n",
    "cipher = read_file(\"data/cipher.txt\")\n",
    "# print(cipher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using `get_statistics`, make sure that `cipher=True` is set when the input is a ciphertext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(content, cipher=True):\n",
    "    stats = {}\n",
    "    content = list(content)\n",
    "    split_content = [x for x in content if x != '\\n' and x!=' ']\n",
    "    length = len(split_content)\n",
    "    symbols = set(split_content)\n",
    "    uniq_sym = len(list(symbols))\n",
    "    freq = collections.Counter(split_content)\n",
    "    rel_freq = {}\n",
    "    for sym, frequency in freq.items():\n",
    "        rel_freq[sym] = (frequency/length)*100\n",
    "        \n",
    "    if cipher:\n",
    "        stats = {'content':split_content, 'length':length, 'vocab':list(symbols), 'vocab_length':uniq_sym, 'frequencies':freq, 'relative_freq':rel_freq}\n",
    "    else:\n",
    "        stats = {'length':length, 'vocab':list(symbols), 'vocab_length':uniq_sym, 'frequencies':freq, 'relative_freq':rel_freq}\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cipher_desc = get_statistics(cipher, cipher=True)\n",
    "plaintxt = read_file(\"data/default.wiki.txt.bz2\")\n",
    "plaintxt_desc = get_statistics(plaintxt, cipher=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the 6-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading language model from data/6-gram-wiki-char.lm.bz2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sequence = 'In a few cases, a multilingual artifact has been necessary to facilitate decipherment, the Rosetta Stone being the classic example. Statistical techniques provide another pathway to decipherment, as does the analysis of modern languages derived from ancient languages in which undeciphered texts are written. Archaeological and historical information is helpful in verifying hypothesized decipherments.'\n",
    "\n",
    "# lm = LM(\"data/6-gram-wiki-char.lm.bz2\", n=6, verbose=True)\n",
    "lm = LM(\"data/6-gram-wiki-char.lm.bz2\", n=6, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for Reference 3 to find the optimal extension order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of the reference 'Beam Search for Solving Substitution Ciphers'. The goal is to find the best extension order. As the paper mentioned, it is important to find a set of weights for the ngram order. I chose the weights \\[1,1,1,1,2,3\\] suggested by Anoop in a discussion post. I also tried several sets of weights. For beamsize of 10000, the result was not influenced quite much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the correct answer has been pruned out at a very early stage, it is not quite possible that the text can be fully deciphered since the following score is computed based on a wrong partial deciphered text. We tried several sets of weights to get different extension order results. But the results are pretty similar in terms of the orders. To try different orders, we pick all the ext_orders with different starting symbols. In the case of Zodiac, there are 5 candidate ext_order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sharp_n(cipher_desc, symbols_found, n_order):\n",
    "    '''\n",
    "    finds the #n for order n_order\n",
    "    cipher_desc -- cipher statistics\n",
    "    symbols_found -- list of single character string,\n",
    "                     specifies the list of symbols have been placed in the extention order\n",
    "    n_order -- int, specifies the order of n-gram\n",
    "    '''\n",
    "    sharp_n = 0\n",
    "    for i in range(len(cipher_desc['content'])-n_order+1):\n",
    "        for j in range(i, i+n_order, 1):\n",
    "            if cipher_desc['content'][j] not in symbols_found:\n",
    "                break\n",
    "            if j == (i+n_order-1):\n",
    "                sharp_n += 1\n",
    "                #print(cipher_desc['content'][i:i+n_order])\n",
    "    return sharp_n            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a beam search to find the optimal extension order. The code below is pretty similar to the beam_search function. Most of the code is copied from it. For simplicity, the variable name might not quite make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ext_order(cipher_desc, topn=100, weights=[1,1,1,1,2,3]):\n",
    "    '''\n",
    "    finds the best order of deciphering cipher symbols (find best extention order)\n",
    "    cipher_desc -- cipher statistics\n",
    "    topn -- int, number of best trees we want to keep during iteration\n",
    "    weights -- list of int, weight for #n, n varies from 1 to 6\n",
    "    '''\n",
    "    # symbols already found with score\n",
    "    Hs = [([], 0)]\n",
    "    # hypothesis extended symbols with score\n",
    "    Ht = []\n",
    "    # initialize the cardinality (number of unique cipher text)\n",
    "    cardinality = 0\n",
    "    # if no weight is specified for unigram, use the most frequent symbol as the starting point\n",
    "    if weights[0] == 0:\n",
    "        cardinality += 1\n",
    "        Hs.append(([sorted(cipher_desc['frequencies'], key=cipher_desc['frequencies'].get, reverse=True)[0]], 0))\n",
    "    # list of cipher symbols\n",
    "    Ve = sorted(cipher_desc['frequencies'], key=cipher_desc['frequencies'].get, reverse=True)\n",
    "    while cardinality < cipher_desc['vocab_length']:\n",
    "        for phi, previous_score in Hs:\n",
    "            for e in Ve:\n",
    "                phi_prime = copy.deepcopy(phi)\n",
    "                if e in phi_prime:\n",
    "                    continue\n",
    "                else:\n",
    "                    phi_prime.append(e)\n",
    "                    this_score = 0\n",
    "                    for i in range(len(weights)):\n",
    "                        this_score += weights[i]*find_sharp_n(cipher_desc, phi_prime, i+1)\n",
    "                    Ht.append((phi_prime, this_score))\n",
    "        # prune the histogram\n",
    "        Ht = sorted(Ht, key=lambda x:x[1], reverse=True)[:topn]                    \n",
    "        cardinality += 1\n",
    "        Hs = copy.deepcopy(Ht)\n",
    "        Ht.clear()\n",
    "#         if cardinality <= 5:\n",
    "#             print(Hs)\n",
    "        #print('Done with symbol number', cardinality, '; Current best score: ', Hs[0][1])\n",
    "        #print('Hs', Hs)\n",
    "    return sorted(Hs, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ext_orders = find_ext_order(cipher_desc, topn=100, weights=[1,1,1,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_order = ['E', '∑', 'B', 'P', 'º', '∫', '/', 'Z', 'A',\n",
    " '∆', 'u', 'O', 'R', 'À', 'V', '–', '•', 'X',\n",
    " 'W', '√', '§', 'F', 'H', '≈', '—', 'π', '+',\n",
    " 'G', 'D', 'K', 'y', '∞', 'I', 'æ', 'µ', '∏',\n",
    " 'T', 'ƒ', '“', '\\\\', 'Q', 'J', 'Ç', 'M',\n",
    " '£', 'L', '‘', '^', 'S', 'Ã', 'N', '¢', 'Ω',\n",
    " 'j']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('ext_order.pkl', 'wb') as fh:\n",
    "#     pickle.dump(ext_orders, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('ext_order.pkl', 'rb') as fh:\n",
    "#     ext_order = pickle.load(fh, encoding='utf8')[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline with better extension order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first change I made to the baseline was to rewrite the score function to optimize the running speed. The change I made is to score the newly fixed symbol plantext character pair and corresponding influenced previously fixed plantext character based on the previous score instead of scoring the whole bitstring in each iteration. For instance, 'oooo...o' -> 'oooo..xo'. The new score can be calculated by adding unigram score of 'x' to the previous score, substracting unigram score of 'o' following 'x' and bigram score of '<\\s>' from the previous score, and adding bigram score of 'o' following 'x' and trigram score of '<\\s>' to the previous score. With this approach, the running time was improved to 20 minitues from 1 hour with a beamsize of 10000 on my machine with i7 7700k cpu. And the computed score is almost same as the score computed with score_bit_string function (the difference is within 0.0000001). Another approach to speed up the whole process we tried is multiprocessing. We will talk about that in the following notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(cipher, phi, new_f, new_e, previous_score):\n",
    "    '''\n",
    "    scores the phi_prime based on the previous score, returns a float\n",
    "    cipher -- list of single character string\n",
    "    phi -- dictionary, old mapping e->[f]\n",
    "    new_f -- single-character string, extended symbol\n",
    "    previous_score -- float, old score for phi\n",
    "    '''\n",
    "    mapping = phi\n",
    "    new_score = previous_score\n",
    "    # for the first iteration, the previous score should be -2.545382 instead of 0\n",
    "    # this is because the score of an empty string is not 0 whiling scoring with bitstring\n",
    "    # the value can be obtained by calling lm.score_bitstring('thisisatest', '...........')\n",
    "    if len(phi)==0:\n",
    "        new_score += -2.545382\n",
    "    lm_state = lm.begin()\n",
    "    old_lm_state = lm.begin()\n",
    "    # this Flag is used to track if a newly-fixed character affects the previously-fixed character\n",
    "    triggerChangeFlag = 0\n",
    "    for i in range(len(cipher)):\n",
    "        char = cipher[i]\n",
    "        # if this is a previously fixed character and not influenced by the newly-fixed character\n",
    "        # we only need to track the lm_state and old lm_state, no need to compute the score\n",
    "        if (char in mapping.keys()) and (triggerChangeFlag==0):\n",
    "            token = mapping[char]\n",
    "            ngram = lm_state + (token,)\n",
    "            while len(ngram)> 0:\n",
    "                if ngram in lm.table:\n",
    "                    lm_state = ngram[-lm.history:]\n",
    "                    break\n",
    "                else: #backoff\n",
    "                    ngram = ngram[1:]\n",
    "            if len(ngram)==0:\n",
    "                lm_state = ()\n",
    "            old_lm_state = lm_state\n",
    "        # if this is a previously fixed character and influenced by the newly-fixed character\n",
    "        # substract the old score and add the new score to the previous score.\n",
    "        elif (char in mapping.keys()) and (triggerChangeFlag>0):\n",
    "            token = mapping[char]\n",
    "            old_lm_state, old_logprob = lm.score(old_lm_state, token)\n",
    "            new_score -= old_logprob\n",
    "            lm_state, logprob = lm.score(lm_state, token)\n",
    "            new_score += logprob\n",
    "            triggerChangeFlag -= 1\n",
    "        # if this is a newly-fixed charater, simply add the new score to the previous score\n",
    "        elif char == new_f:\n",
    "            (lm_state, logprob) = lm.score(lm_state, new_e)\n",
    "            new_score += logprob\n",
    "            triggerChangeFlag = 5\n",
    "            old_lm_state = ()\n",
    "        # if this is a unknown character, there is no influence on the previous score\n",
    "        else:\n",
    "            lm_state = ()\n",
    "            old_lm_state = ()\n",
    "            triggerChangeFlag = 0\n",
    "    # treat the end tag '<\\s>' as previously fixed character\n",
    "    if triggerChangeFlag:\n",
    "        new_score -= lm.end(old_lm_state)\n",
    "        new_score += lm.end(lm_state)\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beam_search we implemented is based on the pseudo code mentioned in the assignment. The main change I made was to use customized ext_limit for each plaintext character instead of a general value. The details can be found in the following notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: ['G', 'S', 'Ã', 'æ'], 4, plain freq 8.69449153965917, cipher freq 6.372549019607844\n",
      "b: ['V'], 1, plain freq 1.5775397729346958, cipher freq 2.2058823529411766\n",
      "c: ['≈'], 1, plain freq 3.296629993243273, cipher freq 2.450980392156863\n",
      "d: ['∆', '§'], 2, plain freq 4.111572946751393, cipher freq 1.7156862745098038\n",
      "e: ['Z', '–', 'W', '+', '∞', 'N', 'E'], 7, plain freq 12.140870772361387, cipher freq 13.235294117647058\n",
      "f: ['J', 'Q'], 2, plain freq 2.2396407473659865, cipher freq 2.696078431372549\n",
      "g: ['R'], 1, plain freq 2.0428939941058446, cipher freq 2.941176470588235\n",
      "h: ['M', '£'], 2, plain freq 4.905679325108425, cipher freq 3.9215686274509802\n",
      "i: ['º', 'P', 'u', 'À'], 4, plain freq 7.395771339569833, cipher freq 10.784313725490197\n",
      "j: [], 0, plain freq 0.21989985102697754, cipher freq 0\n",
      "k: ['/'], 1, plain freq 0.6846719094465245, cipher freq 1.4705882352941175\n",
      "l: ['∫', 'B', '∑'], 3, plain freq 4.12837076781324, cipher freq 8.088235294117647\n",
      "m: ['—'], 1, plain freq 2.5998296931383753, cipher freq 3.9215686274509802\n",
      "n: ['O', '^', 'D', '¢'], 4, plain freq 7.342333650329038, cipher freq 5.637254901960785\n",
      "o: ['X', 'Ç', 'T', 'ƒ'], 4, plain freq 7.219970316976703, cipher freq 6.61764705882353\n",
      "p: ['•'], 1, plain freq 2.0233066431925706, cipher freq 1.715686274509804\n",
      "q: [], 0, plain freq 0.08921644168299855, cipher freq 0\n",
      "r: ['‘', '“', '\\\\'], 3, plain freq 6.642234427965062, cipher freq 4.6568627450980395\n",
      "s: ['F', 'π', 'K', 'Ω'], 4, plain freq 6.608347704465564, cipher freq 4.901960784313725\n",
      "t: ['H', 'I', 'µ', 'L'], 4, plain freq 8.799329348510154, cipher freq 8.57843137254902\n",
      "u: ['y'], 1, plain freq 2.6139471398647225, cipher freq 2.450980392156863\n",
      "v: ['√'], 1, plain freq 1.0442301789046777, cipher freq 1.4705882352941175\n",
      "w: ['A'], 1, plain freq 1.7312671245295852, cipher freq 1.9607843137254901\n",
      "x: ['j'], 1, plain freq 0.19158005883967444, cipher freq 0.24509803921568626\n",
      "y: ['∏'], 1, plain freq 1.5362668595242495, cipher freq 1.9607843137254901\n",
      "z: [], 0, plain freq 0.12010745268987694, cipher freq 0\n"
     ]
    }
   ],
   "source": [
    "def read_gold(gold_file):\n",
    "    with open(gold_file) as f:\n",
    "        gold = f.read()\n",
    "    f.close()\n",
    "    gold = list(gold.strip())\n",
    "    return gold\n",
    "\n",
    "def symbol_error_rate(dec, _gold):\n",
    "    gold = read_gold(_gold)\n",
    "    correct = 0\n",
    "    if len(gold) == len(dec):\n",
    "        for (d,g) in zip(dec, gold):\n",
    "            if d==g:\n",
    "                correct += 1\n",
    "    wrong = len(gold)-correct\n",
    "    error = wrong/len(gold)\n",
    "    \n",
    "    return error\n",
    "gold = read_gold('data/_ref_Zodiac_408.txt')\n",
    "gold_dict = dict()\n",
    "for index in range(len(cipher_desc['content'])):\n",
    "    cipher_char = cipher_desc['content'][index]\n",
    "    if cipher_char not in gold_dict.keys():\n",
    "        gold_dict[cipher_char] = gold[index]\n",
    "\n",
    "Ve = [chr(i) for i in range(97, 123, 1)]\n",
    "for e in Ve:\n",
    "    keys = [k for k, v in gold_dict.items() if v == e]\n",
    "    freq_cipher = 0\n",
    "    for key in keys:\n",
    "        freq_cipher += cipher_desc['relative_freq'][key]\n",
    "    freq_plain = plaintxt_desc['relative_freq'][e]\n",
    "    print(f'{e}: {keys}, {len(keys)}, plain freq {freq_plain}, cipher freq {freq_cipher}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(cipher, ext_order, score_func, ext_limits, topn=1):\n",
    "    '''\n",
    "    finds the mappings between cipher char and plaintext char, returns the mapping dictionary\n",
    "    ext_order -- list, the unigram char list sorted by their count DESC\n",
    "    ext_limits -- int, defines maximum number of cipher char can be mapped to a plaintext char\n",
    "    topn -- int, defines the number of dictionaries we want to keep while pruning\n",
    "    '''\n",
    "    print('Number of unique symbols in cipher:', len(ext_order))\n",
    "    # mapping relationships already found with score\n",
    "    Hs = [(defaultdict(dict), 0)]\n",
    "    # hypothesis mapping relationships with score\n",
    "    Ht = []\n",
    "    # initialize the cardinality (number of unique cipher text)\n",
    "    cardinality = 0\n",
    "    # list of plaintext characters\n",
    "    Ve = [chr(i) for i in range(97, 123, 1)]\n",
    "    while cardinality < len(ext_order):\n",
    "        f = ext_order[cardinality]\n",
    "        print('Working on symbol: ', f, f'({cardinality+1})')\n",
    "        for phi, previous_score in Hs:\n",
    "            for e in Ve:\n",
    "                phi_prime = copy.deepcopy(phi)\n",
    "                new_map = {f: e}\n",
    "                phi_prime.update(new_map)\n",
    "                counts = len([v for k, v in phi_prime.items() if v == e])\n",
    "                ext_limit = ext_limits[e]\n",
    "                if counts <= ext_limit:\n",
    "                    Ht.append((phi_prime, score_func(cipher, phi, f, e, previous_score)))\n",
    "        # prune the histogram\n",
    "        Ht = sorted(Ht, key=lambda x:x[1], reverse=True)[:topn]\n",
    "        cardinality += 1\n",
    "        Hs = copy.deepcopy(Ht)\n",
    "        Ht.clear()\n",
    "        \n",
    "        #######################    monitor if the correct result has been pruned out #######################\n",
    "        print('Best score: ', Hs[0][1], 'Worst score: ', Hs[min(len(Hs)-1, topn-1)][1])\n",
    "        found_symbols = ext_order[:cardinality]\n",
    "        phi_temp = dict()\n",
    "        for key,value in gold_dict.items():\n",
    "            if key in found_symbols:\n",
    "                phi_temp[key] = value\n",
    "        partial_text = ''\n",
    "        bit_string = ''\n",
    "        for cipher_char in cipher_desc['content']:\n",
    "            if cipher_char in phi_temp.keys():\n",
    "                partial_text += phi_temp[cipher_char]\n",
    "                bit_string += 'o'\n",
    "            else:\n",
    "                partial_text += '_'\n",
    "                bit_string += '.'\n",
    "        gold_score = lm.score_bitstring(partial_text, bit_string)\n",
    "        print('gold score', gold_score)\n",
    "        if gold_score < Hs[min(len(Hs)-1, topn-1)][1] or gold_score > Hs[0][1]:\n",
    "            print('Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!')\n",
    "        \n",
    "    return sorted(Hs, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deciphering the Zodiac Killer cipher, test the algorithm with some simple test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first test case is a simple one to one mapping. <br>\n",
    "Plaintext: `defendtheeastwallofthecastle` <br>\n",
    "Cipher: `giuifgceiiprctpnnduceiqprcni` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_to_one_cipher = 'giuifgceiiprctpnnduceiqprcni'\n",
    "one_to_one_cipher_desc = get_statistics(one_to_one_cipher, cipher=True)\n",
    "one_to_one_ext_order = find_ext_order(one_to_one_cipher_desc)[0][0]\n",
    "one_to_one_ext_limits = dict()\n",
    "for e in [chr(i) for i in range(97, 123, 1)]:\n",
    "    one_to_one_ext_limits[e] = 1\n",
    "one_to_one_mappings = beam_search(one_to_one_cipher_desc['content'], one_to_one_ext_order,\\\n",
    "                                  score, one_to_one_ext_limits, 5000)\n",
    "one_to_one_mapping = one_to_one_mappings[0][0]\n",
    "one_to_one_decipher_text = ''\n",
    "for char in one_to_one_cipher_desc['content']:\n",
    "    one_to_one_decipher_text += one_to_one_mapping[char]\n",
    "print('Deciphered result: ', one_to_one_decipher_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't come up with a homophobic cipher. The reason is that our algorithm is able to decipher the Zodiac with a beamsize of 10000 after fixing three ground true mapping relationships (result not shown). The symbol error rate for that is ~5%. With a beamsize of 10000, the algorithm cannot correctly decipher the text all by its own. But it is enough to prove the baseline algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_fn(Ve, phi, f, cipher, previous_score, ext_limits):\n",
    "    ret = []\n",
    "    for e in Ve:\n",
    "        phi_prime = copy.deepcopy(phi)\n",
    "        new_map = {f: e}\n",
    "        phi_prime.update(new_map)\n",
    "        counts = len([v for k, v in phi_prime.items() if v == e])\n",
    "        if counts <= ext_limits[e]:\n",
    "            ret.append((phi_prime, score(cipher, phi, f, e, previous_score)))\n",
    "    return ret\n",
    "\n",
    "def beam_search_mp(cipher, ext_order, ext_limits, topn=1):\n",
    "\n",
    "    # initialization\n",
    "    Hs = [(defaultdict(dict), 0)]\n",
    "    Ht = []\n",
    "    cardinality = 0\n",
    "    Ve = [chr(i) for i in range(97, 123, 1)]\n",
    "    \n",
    "    while cardinality < len(ext_order):\n",
    "        f = ext_order[cardinality]\n",
    "        print('Working on symbol: ', f, f'({cardinality+1})')\n",
    "        \n",
    "        mainStart = time.time()\n",
    "        result = []\n",
    "        p = Pool(cpu_count())\n",
    "             \n",
    "        for phi, previous_score in Hs: \n",
    "            result.append(p.apply_async(parallel_fn, args=(Ve, phi, f, cipher, previous_score, ext_limits))) \n",
    "                            \n",
    "        p.close() \n",
    "        p.join()  \n",
    "\n",
    "        Ht = []\n",
    "        for subp in result:\n",
    "            Ht += subp.get()\n",
    "            \n",
    "        if cardinality < 10 or cardinality >= 40:\n",
    "            topn = 100000\n",
    "        else:\n",
    "            topn = 1000000\n",
    "    \n",
    "        # prune the histogram\n",
    "        mainEnd = time.time()\n",
    "        print ('Running Time for this symbol: %0.2f seconds.' % (mainEnd-mainStart))\n",
    "        Ht = sorted(Ht, key=lambda x:x[1], reverse=True)[:topn]    \n",
    "        \n",
    "        cardinality += 1\n",
    "        Hs = copy.deepcopy(Ht)\n",
    "        Ht.clear()\n",
    "        \n",
    "        #######################    monitor if the correct result has been pruned out #######################\n",
    "        print('Best score: ', Hs[0][1], 'Worst score: ', Hs[min(len(Hs)-1, topn-1)][1])\n",
    "        found_symbols = ext_order[:cardinality]\n",
    "        phi_temp = dict()\n",
    "        for key,value in gold_dict.items():\n",
    "            if key in found_symbols:\n",
    "                phi_temp[key] = value\n",
    "        partial_text = ''\n",
    "        bit_string = ''\n",
    "        for cipher_char in cipher_desc['content']:\n",
    "            if cipher_char in phi_temp.keys():\n",
    "                partial_text += phi_temp[cipher_char]\n",
    "                bit_string += 'o'\n",
    "            else:\n",
    "                partial_text += '_'\n",
    "                bit_string += '.'\n",
    "        gold_score = lm.score_bitstring(partial_text, bit_string)\n",
    "        print('gold score', gold_score)\n",
    "        if gold_score < Hs[min(len(Hs)-1, topn-1)][1] or gold_score > Hs[0][1]:\n",
    "            print('Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!')\n",
    "        \n",
    "    return sorted(Hs, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decipher Zodiac Killer cipher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we use different ext_limit for different plaintext symbol. We use the following `ext_limits` to limit the ext_order. The goal is to customize the `ext_limit` for each plaintext character so that more symbols can be mapped to the more frequent plaintext character. The `ext_limit` is calculated by multiplying the relative frequency of the paintext character in wiki text by the number of unique cipher symbols. We believe this can dramatically improve the running time since the required amount of computation in each iteration is minimized. We use the ceiling instead of floor to ensure the `ext_limit` is large enough. Of course this approach might not be perfect if less frequent plaintext character is mapped to more cipher symbols. But even that is the case, the error rate should not increase significantly since the frequency of those less frequent plaintext should not be large (if the character distribution of the plaintext in the cipher task is similar to that of the one used to train the language model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 5, 'b': 1, 'c': 2, 'd': 3, 'e': 7, 'f': 2, 'g': 2, 'h': 3, 'i': 4, 'j': 1, 'k': 1, 'l': 3, 'm': 2, 'n': 4, 'o': 4, 'p': 2, 'q': 1, 'r': 4, 's': 4, 't': 5, 'u': 2, 'v': 1, 'w': 1, 'x': 1, 'y': 1, 'z': 1}\n"
     ]
    }
   ],
   "source": [
    "ext_limits = dict()\n",
    "for e in [chr(i) for i in range(97, 123, 1)]:\n",
    "    ext_limits[e] = math.ceil(plaintxt_desc['relative_freq'][e]*cipher_desc['vocab_length']/100)\n",
    "print(ext_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique symbols in cipher: 54\n",
      "Working on symbol:  E (1)\n",
      "Best score:  -10.7856218 Worst score:  -29.749483999999995\n",
      "gold score -10.785621800000001\n",
      "Working on symbol:  ∑ (2)\n",
      "Best score:  -20.517739599999995 Worst score:  -59.14061499999999\n",
      "gold score -24.181161000000003\n",
      "Working on symbol:  B (3)\n",
      "Best score:  -32.11996419999999 Worst score:  -57.93098\n",
      "gold score -38.09201220000001\n",
      "Working on symbol:  P (4)\n",
      "Best score:  -40.82754000000006 Worst score:  -53.75988140000001\n",
      "gold score -48.728844000000024\n",
      "Working on symbol:  º (5)\n",
      "Best score:  -54.011017099999975 Worst score:  -64.64988409999995\n",
      "gold score -62.340047100000035\n",
      "Working on symbol:  ∫ (6)\n",
      "Best score:  -66.89316288000005 Worst score:  -75.60772610000008\n",
      "gold score -74.86408880000002\n",
      "Working on symbol:  / (7)\n",
      "Best score:  -72.68006079000003 Worst score:  -80.948034\n",
      "gold score -82.52081120000001\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  Z (8)\n",
      "Best score:  -81.55473529000004 Worst score:  -88.53046970000003\n",
      "gold score -88.05795994000003\n",
      "Working on symbol:  A (9)\n",
      "Best score:  -89.06488246800014 Worst score:  -96.1867459\n",
      "gold score -98.26804168000001\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  ∆ (10)\n",
      "Best score:  -92.35546822000003 Worst score:  -98.96608780000003\n",
      "gold score -100.32490010000001\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  u (11)\n",
      "Best score:  -102.84589982000004 Worst score:  -109.48829635000008\n",
      "gold score -108.72940661000003\n",
      "Working on symbol:  O (12)\n",
      "Best score:  -108.45606159000006 Worst score:  -115.82968350700013\n",
      "gold score -114.78476954\n",
      "Working on symbol:  R (13)\n",
      "Best score:  -122.02985343000013 Worst score:  -129.48827501000008\n",
      "gold score -131.19456633999997\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  À (14)\n",
      "Best score:  -130.18600501000003 Worst score:  -138.29487074000002\n",
      "gold score -141.07347840999995\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  – (15)\n",
      "Best score:  -137.01676563000007 Worst score:  -143.33284166000004\n",
      "gold score -147.22324317299993\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  V (16)\n",
      "Best score:  -144.63341993000003 Worst score:  -152.7761058799999\n",
      "gold score -159.95173187299994\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  • (17)\n",
      "Best score:  -152.04066573000006 Worst score:  -158.52304264000026\n",
      "gold score -171.19284527299996\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  X (18)\n",
      "Best score:  -162.11271138999993 Worst score:  -168.33093613000005\n",
      "gold score -180.16133218929997\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  W (19)\n",
      "Best score:  -171.4603068899998 Worst score:  -178.0481971199999\n",
      "gold score -186.21911809217994\n",
      "Wrong Wrong Wrong Wrong Wrong Wrong Wrong Wrong!\n",
      "Working on symbol:  √ (20)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # single core version\n",
    "# mappings = beam_search(cipher_desc['content'], ext_order, score, ext_limits, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping = mappings[0][0]\n",
    "# decipher_text = ''\n",
    "# for char in cipher_desc['content']:\n",
    "#     decipher_text += mapping[char]\n",
    "# print(decipher_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# multiprocessing version\n",
    "mappings = beam_search_mp(cipher_desc['content'], ext_order, ext_limits, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = mappings[0][0]\n",
    "decipher_text = ''\n",
    "for char in cipher_desc['content']:\n",
    "    decipher_text += mapping[char]\n",
    "print(decipher_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
