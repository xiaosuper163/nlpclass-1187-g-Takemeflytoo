### username: yabing

* The work I have done for the final submission:
  * Implement the approaches of generating rules for S2. For more details around how this approach works, please see the jupyter notebook `'cgw.ipynb'`.

* The experiments I have tried but not included in the final submission:
  * Go through each sentence from example_sentences.txt and manually add rules to S1 and S2. The corresponding files `'S1_yabing.gr'`, `'S2_yabing.gr'`, `'Vocab_yabing.gr'` are in the branch `'yabing'` (not merged to master to keep the master clean). It turns out the performance (cross entropy) with this approach is not as good as the final submission. We decided not to go with this one. The corresponding python jupyter notebook, 'cgw-default.ipynb', is in branch `'yabing'`.
  * Use a parser described in "Constituency Parsing with a Self-Attentive Encoder" from ACL 2018 to generate tree for each sentence from `example_sentences.txt` and `devset.txt`. Then I generate the eCNF with NLTK built-in function `chomsky_normal_form`. After going through all sentences, we run another pass to update the weight of each rule. So basically there are two passes. During the first pass, we find all the rules. And during the second pass, we update the weight of the rules. This approach is very similar to the approach used by another group member, Katrina. The performance on devset.txt with this approach is much worse than the final submimssion. In this experiment, I implemented the whole pipeline which was used by myself. Since Katrina also tried this method, there might be some repeated work. The corresponding python jupyter notebook, `'pipeline.ipynb'`, is in branch `'yabing'`.