{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pcfg_parse_gen import Pcfg, PcfgGenerator, CkyParse\n",
    "import nltk\n",
    "import benepar\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en to\n",
      "[nltk_data]     /Users/junbo/nltk_data...\n",
      "[nltk_data]   Package benepar_en is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/junbo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/junbo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benepar.download('benepar_en')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tags and assign weights for all allowed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**<br>\n",
    ">**1. Generate parsed trees using \"benepar\" parser based on devset and other sentences we found**<br>\n",
    ">**2. For each word in allowed words, find tags in the parsed trees and count the occurances of the tags**<br>\n",
    ">**3. Manually assign the tags to those words that appeared in allowed words but not in the parsed trees**<br>\n",
    ">**4. Fix the weights of each word**<br>\n",
    ">**5. Manually fix the format and the tags for punctuations (e.g. replace left hand side \".\" to END)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine devset and the new sentences we found into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "allQuotes = []\n",
    "file3 = open('devset.txt', 'r')\n",
    "file4 = open('quotes_new_preprocessed.txt', 'r')\n",
    "for i in file3:\n",
    "    allQuotes.append(i.strip().split(' '))\n",
    "for j in file4:\n",
    "    allQuotes.append(list(filter(None, j.strip().split(' '))))\n",
    "file3.close()\n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate parsed trees using \"benepar\" parser, and write to allTrees.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_parser = benepar.Parser(\"benepar_en\")\n",
    "count = 0\n",
    "file5 = open('allTrees.txt', 'w')\n",
    "for sent in allQuotes:\n",
    "    file5.write(str(bst_parser.parse(sent)))\n",
    "    file5.write(\"\\n\")\n",
    "    count+=1\n",
    "    print(count)\n",
    "    print(sent)\n",
    "file5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in allowed words, find and keep all associate tags in the parsed trees. <br>\n",
    "If a word cannot be found in the parsed trees, then assign tag Misc and weight 1 to that word. <br>\n",
    "If only one tag found for a word, then assign weight 1 to it. <br>\n",
    "If multiple tags found for a word, then count the occurance of each tag, and assign the count as the weight. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file6 = open('allTrees.txt', 'r')\n",
    "file7 = open('allowed_words.txt', 'r')\n",
    "file8 = open('tags.txt', 'w')\n",
    "allTrees = file6.read()\n",
    "all_tags = set()\n",
    "for word in file7:\n",
    "    tag_line = '1    '\n",
    "    wrd1 = word.strip()\n",
    "    tags = list()\n",
    "    if (wrd1 != '(') and (wrd1 != ')'):\n",
    "        for i in re.findall(\"\\([^\\s]+ \"+str(wrd1), allTrees):\n",
    "            tg = i.split(\" \")[0].replace(\"(\",'')\n",
    "            if tg != '':\n",
    "                tags.append(tg)\n",
    "    else:\n",
    "        if wrd1 == '(':\n",
    "            tag_line+=(\"(\")\n",
    "        else:\n",
    "            tag_line+=(\")\")\n",
    "        tag_line+='    '\n",
    "        \n",
    "        tag_line+= wrd1\n",
    "        file8.write(tag_line)\n",
    "        file8.write(\"\\n\") \n",
    "        continue\n",
    "    \n",
    "    if (len(tags)==0) and (wrd1 != '(') and (wrd1 != ')'):\n",
    "        tag_line+=(\"Misc\")\n",
    "        tag_line+='    '\n",
    "        tag_line+= wrd1\n",
    "        file8.write(tag_line)\n",
    "        file8.write(\"\\n\")\n",
    "    else:\n",
    "        all_tags = all_tags.union(set(tags))\n",
    "        for j in list(set(tags)):\n",
    "            tag_line = str(tags.count(j))\n",
    "            if len(set(tags))==1:\n",
    "                tag_line = str(1)\n",
    "            tag_line+= '    '\n",
    "            tag_line+=(j)\n",
    "            tag_line+='    '\n",
    "            tag_line+= wrd1\n",
    "            file8.write(tag_line)\n",
    "            file8.write(\"\\n\")\n",
    "    \n",
    "file6.close()\n",
    "file7.close()\n",
    "file8.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word, convert the weight of each tag to percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagsDF = pd.read_csv('tags.txt', header=None, sep='    ', engine='python', names=['ct','tag','word'])\\\n",
    ".sort_values(by=['tag']).reset_index(drop=True)\n",
    "\n",
    "merged = tagsDF.groupby('word').sum().rename(columns={'ct':'summed'}).reset_index().merge(tagsDF)\n",
    "\n",
    "merged['weight'] = (merged['ct']/merged['summed']*100).astype(int)\n",
    "\n",
    "weights = []\n",
    "for i in merged['weight']:\n",
    "    if i==100:\n",
    "        weights.append(i)\n",
    "    else:\n",
    "        weights.append(i+1)\n",
    "\n",
    "merged['weight'] = weights\n",
    "\n",
    "merged[['weight', 'tag', 'word']].to_csv('tags_weight.gr', header=False, index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
