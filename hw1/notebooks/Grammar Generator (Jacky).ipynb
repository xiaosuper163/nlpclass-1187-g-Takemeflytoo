{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import benepar\n",
    "# https://github.com/nikitakit/self-attentive-parser\n",
    "import nltk\n",
    "from nltk.tree import Tree, ParentedTree\n",
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "import codecs\n",
    "import nltk.tokenize.punkt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en to\n",
      "[nltk_data]     C:\\Users\\jacky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jacky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jacky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benepar.download('benepar_en')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag for Special Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "    \n",
    "def set_punc(s):\n",
    "    if s[0] in set(string.punctuation) and not (s[0]==\"'\" and len(s) > 1):\n",
    "        if s in ['(']:\n",
    "            return 'BGNBK'\n",
    "        if s in [')']:\n",
    "            return 'ENDBK'\n",
    "        if s in ['\"', \"'\"]:\n",
    "            return 'QUOTE'\n",
    "        if re.match('`+$', s):\n",
    "            return 'QUOTE'\n",
    "        if s in [':',';']:\n",
    "            return 'BREAK'\n",
    "        if s in ['.','!','?']:\n",
    "            return 'END'\n",
    "        if s in [',']:\n",
    "            return 'PAUSE'\n",
    "        if re.match('(\\.\\.+)$|(-+)$', s):\n",
    "            return 'BREAK'\n",
    "    return False\n",
    "\n",
    "def set_number(s):\n",
    "    # In our allow_words, this is good enough\n",
    "    if s[0].isdigit():\n",
    "        return 'CD'\n",
    "    return False\n",
    "\n",
    "def set_shorten(s):\n",
    "    if s[0] == \"'\" and len(s) > 1:\n",
    "        if s == \"'ow\":\n",
    "            return 'RB'\n",
    "        if s == \"'em\":\n",
    "            return 'PRP'\n",
    "        return 'PRP'\n",
    "    return False\n",
    "\n",
    "def set_UTs(s):\n",
    "    if s == 'y':\n",
    "        return 'PRP'\n",
    "    UT_list = [\n",
    "        '[A|a][a|u|g]*h+$', # Aaaaagh, Aaauggh, Aah ... \n",
    "        'u[u|g|h|m|n]+$', # ug, uh, um ..\n",
    "        'Noo', \n",
    "        'Oo[h|f|o]*$', # Ooh, Oof ..\n",
    "        '[O|o]+[u|i|w|l|p]*$',\n",
    "        'e+m*$',\n",
    "        '([H|h][a|e|h|o|y|l]+)$~[H|h]e[ll]*$', # Hello, Hallo, Holy, Hee ...\n",
    "        '[S|s]h+$', # Shh\n",
    "        'whoa',\n",
    "        '[Y|y]*[E|e]*$' # Yee, ye ...\n",
    "    ]\n",
    "    cond = False\n",
    "    for exp in UT_list:\n",
    "        extra = True\n",
    "        if '~' in exp:\n",
    "            sp = exp.split('~')\n",
    "            exp = sp[0]\n",
    "            extra = not (re.match(sp[1], s))\n",
    "        cond = ((re.match(exp, s)) and (extra)) or (cond)\n",
    "    if cond:\n",
    "        return 'UT'\n",
    "    return False\n",
    "\n",
    "def correct_tag(text, old_tag):\n",
    "    for test in [set_punc, set_number, set_shorten, set_UTs]:\n",
    "        new_tag = test(text)\n",
    "        if new_tag and new_tag!= old_tag:\n",
    "            return new_tag\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'END'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tag(\".\", 'CD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenRules:\n",
    "    \n",
    "    __slot__ = ['parser', 'allowed_words', 'paths' , '__cache']\n",
    "    \n",
    "    \"\"\"\n",
    "    sents -- list of strings, a list of sentences\n",
    "    parser -- a nltk parser, benepar in this case\n",
    "    allowed_words -- list of strings, this is used to identify which are grammars and which are vocabs\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, parser=\"stanfordcorenlp\"):\n",
    "        self.allowed_words = self.load_allowed_words()\n",
    "        self.parser = parser\n",
    "        self.paths = paths\n",
    "        self.sents = self.load_sents(paths)\n",
    "        self.__cache = {}\n",
    "        self.missed = {}\n",
    "        if parser == \"stanfordcorenlp\":\n",
    "            self.parser = self.stanford_parser()\n",
    "        else:\n",
    "            self.parser = benepar.Parser(\"benepar_en\")\n",
    "        \n",
    "    def stanford_parser(self):\n",
    "        from stanfordcorenlp import StanfordCoreNLP\n",
    "        # https://stanfordnlp.github.io/CoreNLP/\n",
    "        return StanfordCoreNLP(r'C:\\tools\\stanford-corenlp-full-2018-02-27')\n",
    "        \n",
    "    def load_allowed_words(self):\n",
    "        # with open('allowed_words.txt', 'r') as fh:\n",
    "        #    return [line.strip() for line in fh]\n",
    "        return pd.read_csv('Tagged_Vocab.gr', sep='\\ ', comment='#', header=None, \n",
    "                           engine='python', names=['p','tag','words'])\n",
    "        \n",
    "    def load_sents(self, paths=[]):\n",
    "        tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "        if isinstance(paths, str):\n",
    "            paths = [paths]\n",
    "        if len(paths) > 0:\n",
    "            text = \"\"\n",
    "            for path in paths:\n",
    "                text += codecs.open(path, \"r\" , \"utf8\").read()\n",
    "        return tokenizer.tokenize(text.strip())\n",
    "    \n",
    "    def parse(self, sents, paths):\n",
    "        \"\"\" Parse sentances with selected parser\n",
    "            Cache it if possible\n",
    "        \"\"\"\n",
    "        if str(paths) in CACHE:\n",
    "            return CACHE[str(paths)]\n",
    "        result = []\n",
    "        for sent in sents:\n",
    "            try:\n",
    "                r = self.parser.parse(sent)\n",
    "                if isinstance(r, str):\n",
    "                    r = Tree.fromstring(r)\n",
    "                    r.chomsky_normal_form(horzMarkov=2)\n",
    "                    r = ParentedTree.convert(r)\n",
    "                    result.append(r)\n",
    "                if isinstance(r, nltk.tree.Tree):\n",
    "                    r.chomsky_normal_form(horzMarkov=2)\n",
    "                    r = ParentedTree.convert(r)\n",
    "                    result.append(r)\n",
    "            except:\n",
    "                print(sent)\n",
    "        CACHE[str(paths)] = result\n",
    "        return result\n",
    "    \n",
    "    def replace_tag(self, tree):\n",
    "        \"\"\" Replace tree tag\n",
    "        \"\"\"\n",
    "        replace_dict = {}\n",
    "        aw = self.allowed_words\n",
    "        for i in tree.pos():\n",
    "            new_tag = correct_tag(i[0], i[1])\n",
    "            if not aw[(aw['words']==i[0]) & ((aw['tag']==i[1]) | (aw['tag']==new_tag))].values.any():\n",
    "                if not aw[aw['words']==i[0]].values.any():\n",
    "                    continue\n",
    "                self.missed[i] = tree\n",
    "                new_tag = aw[aw['words']==i[0]].sort_values(['p'], ascending=False)['tag'].values[0]\n",
    "            if new_tag:\n",
    "                replace_dict[i] = new_tag\n",
    "        if len(replace_dict) > 0:\n",
    "            # print(tree)\n",
    "            self.traverse_replace(tree, replace_dict)\n",
    "            # print(tree)\n",
    "    \n",
    "    def traverse_replace(self, tree, replace_dict):\n",
    "        for index, subtree in enumerate(tree):\n",
    "            if isinstance(subtree, str):\n",
    "                return\n",
    "            if subtree.height() == 2:\n",
    "                pos_tuple = subtree.pos()[0]\n",
    "                if pos_tuple in replace_dict:\n",
    "                    old_tag = pos_tuple[1]\n",
    "                    new_tag = replace_dict[pos_tuple]\n",
    "                    # Replace tag\n",
    "                    subtree.set_label(new_tag)\n",
    "                    parent_tag = subtree.parent().label()\n",
    "                    right_sibling = subtree.right_sibling()\n",
    "                    left_sibling = subtree.left_sibling()\n",
    "                    if right_sibling is None and left_sibling is None:\n",
    "                        pass\n",
    "                    elif right_sibling is None: # At right side\n",
    "                        old_tag = '-' + old_tag \n",
    "                        new_tag = '-' + new_tag\n",
    "                    elif left_sibling is None: # At left side\n",
    "                        old_tag += '-'\n",
    "                        new_tag += '-'\n",
    "                    else: # In middle\n",
    "                        old_tag = '-' + old_tag + '-'\n",
    "                        new_tag = '-' + new_tag + '-'\n",
    "                    subtree.parent().set_label(parent_tag.replace(old_tag, new_tag))\n",
    "            if isinstance(subtree, nltk.tree.Tree):\n",
    "                self.traverse_replace(subtree, replace_dict)\n",
    "        \n",
    "    def find_rules(self):\n",
    "        '''\n",
    "        find the rules from the trees parsed PennTree parser\n",
    "        \n",
    "        exclusion_dict -- dictionary, contains the rules we want to exclude in S2\n",
    "                          It is not none when isS1 is False\n",
    "        '''\n",
    "        sents_size = len(self.sents)\n",
    "        print(f'================{sents_size} sentences in total================')\n",
    "        num_finished = 0\n",
    "        counts = defaultdict(lambda : defaultdict(int))\n",
    "        isHeadFlag = True\n",
    "        for tree in self.parse(self.sents, self.paths):\n",
    "            self.replace_tag(tree)\n",
    "                \n",
    "            for production in tree.productions():\n",
    "                counts[production.lhs().symbol()][production] += 1\n",
    "            \n",
    "            if not (num_finished+1) % int(sents_size*0.1):\n",
    "                print(f'================{num_finished+1} sentences finished================')\n",
    "            num_finished += 1\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================1304 sentences in total================\n",
      "Uh , so , uh , anything you can do to , uh , to help , would be ...\n",
      "very ...\n",
      "helpful ...\n",
      "Look , can you tell us wh - Fine , um , I do n't want to waste anymore of your time , but , uh I do n't suppose you could , uh , tell us where we might find a , um , find a , uh , a , um , a uh -- A what ... ?\n",
      "================130 sentences finished================\n",
      "================260 sentences finished================\n",
      "================390 sentences finished================\n",
      "================520 sentences finished================\n",
      "================650 sentences finished================\n",
      "================780 sentences finished================\n",
      "================910 sentences finished================\n",
      "================1040 sentences finished================\n",
      "================1170 sentences finished================\n",
      "================1300 sentences finished================\n",
      "================1430 sentences finished================\n",
      "================1560 sentences finished================\n",
      "================1690 sentences finished================\n",
      "================1820 sentences finished================\n",
      "================1950 sentences finished================\n",
      "================2080 sentences finished================\n",
      "================2210 sentences finished================\n",
      "================2340 sentences finished================\n",
      "================2470 sentences finished================\n",
      "================2600 sentences finished================\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with open('example_sentences.txt') as fh:\n",
    "#     sents = [line.strip() for line in fh]\n",
    "g = GenRules(['example_sentences.txt',  'devset.txt', 'quotes_new_preprocessed.txt'])\n",
    "s1_dict = g.find_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Whoa', 'RB')\n",
      "('Saxons', 'NNPS')\n",
      "('ridden', 'VBN')\n",
      "('bangin', 'JJ')\n",
      "('covered', 'VBD')\n",
      "('found', 'VBD')\n",
      "('Found', 'VB')\n",
      "('fly', 'VB')\n",
      "('matter', 'VB')\n",
      "('second', 'NN')\n",
      "('yeah', 'JJ')\n",
      "('agree', 'VBP')\n",
      "('use', 'VB')\n",
      "('standard', 'JJ')\n",
      "('next', 'JJ')\n",
      "('Man', 'NNP')\n",
      "('object', 'VBP')\n",
      "('treat', 'VB')\n",
      "('AM', 'VBP')\n",
      "('outdated', 'JJ')\n",
      "('imperialist', 'JJ')\n",
      "(\"d'\", 'VB')\n",
      "('syndicalist', 'JJ')\n",
      "('take', 'VBP')\n",
      "('act', 'VB')\n",
      "('executive', 'JJ')\n",
      "('biweekly', 'JJ')\n",
      "('order', 'VBP')\n",
      "('Order', 'NNP')\n",
      "('vote', 'VB')\n",
      "('held', 'VBD')\n",
      "('Well', 'UH')\n",
      "('cause', 'VB')\n",
      "('put', 'VB')\n",
      "('shut', 'VB')\n",
      "('away', 'RP')\n",
      "('saw', 'VBD')\n",
      "('fight', 'VBP')\n",
      "('Court', 'NN')\n",
      "('quarrel', 'VBP')\n",
      "('move', 'VBP')\n",
      "('pansy', 'VBP')\n",
      "('mine', 'JJ')\n",
      "('left', 'VBD')\n",
      "('ere', 'FW')\n",
      "('triumphs', 'VBZ')\n",
      "('burn', 'VB')\n",
      "('dressed', 'VBD')\n",
      "('dress', 'VB')\n",
      "('makes', 'VBZ')\n",
      "('yeah', 'RB')\n",
      "('Great', 'JJ')\n",
      "('lead', 'VB')\n",
      "('dub', 'VBP')\n",
      "('Pure', 'NNP')\n",
      "('stood', 'VBN')\n",
      "('up', 'IN')\n",
      "('formed', 'VBD')\n",
      "('retold', 'VBN')\n",
      "('learning', 'NN')\n",
      "('amazes', 'VBZ')\n",
      "('bid', 'VBP')\n",
      "('welcome', 'VB')\n",
      "('dance', 'VBP')\n",
      "(\"e'er\", 'NN')\n",
      "('dine', 'VBP')\n",
      "('eat', 'VBP')\n",
      "('Though', 'NNP')\n",
      "('That', 'WDT')\n",
      "('fat', 'JJ')\n",
      "('Quite', 'JJ')\n",
      "('sequin', 'VBP')\n",
      "('Gable', 'NNP')\n",
      "('push', 'VB')\n",
      "('try', 'VBP')\n",
      "('talk', 'VB')\n",
      "('forgive', 'VB')\n",
      "('depressing', 'JJ')\n",
      "('Master', 'NNP')\n",
      "('Ruiz', 'NNP')\n",
      "('de', 'FW')\n",
      "('lu', 'FW')\n",
      "('la', 'FW')\n",
      "('Ramper', 'FW')\n",
      "('Uh', 'NNP')\n",
      "('English', 'JJ')\n",
      "('Mind', 'VB')\n",
      "('boil', 'VB')\n",
      "('blow', 'VBP')\n",
      "('fart', 'VBP')\n",
      "('la', 'NNP')\n",
      "('vache', 'NNP')\n",
      "('wha', 'SYM')\n",
      "('wait', 'VBP')\n",
      "('leaps', 'VBZ')\n",
      "('Pictures', 'NNP')\n",
      "('Schools', 'NNP')\n",
      "('disheartened', 'JJ')\n",
      "('decided', 'VBD')\n",
      "('separate', 'VB')\n",
      "('split', 'VBD')\n",
      "('smashed', 'VBD')\n",
      "('removed', 'VBD')\n",
      "('bottom', 'JJ')\n",
      "('enough', 'JJ')\n",
      "('dropped', 'VBN')\n",
      "('j', 'SYM')\n",
      "('fight', 'VB')\n",
      "('ust', 'NN')\n",
      "('snore', 'VBP')\n",
      "('brush', 'VB')\n"
     ]
    }
   ],
   "source": [
    "for k, v in g.missed.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class t:\n",
    "    def __init__ (self, total=0, count=0):\n",
    "        self.total = total\n",
    "        self.count = count\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        self.total += other.total\n",
    "        self.count += other.count\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"({self.total}, {self.count})\"\n",
    "\n",
    "total = defaultdict(t)\n",
    "for k, production in s1_dict.items():\n",
    "    for prod, count in sorted(production.items(), key=lambda x: x[1], reverse=True):\n",
    "        if isinstance(prod.rhs()[0], nltk.grammar.Nonterminal):\n",
    "            total[k] += t(count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {S -> NP S|<VP-END>: 548,\n",
       "             S -> S S|<VP-END>: 6,\n",
       "             S -> VP: 464,\n",
       "             S -> NP VP: 638,\n",
       "             S -> VP END: 228,\n",
       "             S -> S S|<CC-S>: 18,\n",
       "             S -> INTJ S|<PAUSE-NP>: 136,\n",
       "             S -> S VP: 8,\n",
       "             S -> FRAG S|<PAUSE-NP>: 4,\n",
       "             S -> NP ADJP: 34,\n",
       "             S -> SBAR S|<PAUSE-NP>: 24,\n",
       "             S -> S S|<BREAK-SQ>: 4,\n",
       "             S -> CC S|<NP-VP>: 42,\n",
       "             S -> FRAG S|<BREAK-S>: 12,\n",
       "             S -> IN S|<NP-VP>: 10,\n",
       "             S -> NP-TMP S|<PAUSE-NP>: 26,\n",
       "             S -> NP: 32,\n",
       "             S -> S S|<PAUSE-NP>: 60,\n",
       "             S -> S S|<BREAK-S>: 60,\n",
       "             S -> NP S|<PAUSE-NP>: 26,\n",
       "             S -> NP S|<QUOTE-NP>: 2,\n",
       "             S -> QUOTE NP: 2,\n",
       "             S -> ADVP S|<PAUSE-NP>: 44,\n",
       "             S -> S S|<PAUSE-''>: 2,\n",
       "             S -> X S|<X-NP>: 2,\n",
       "             S -> SBAR VP: 2,\n",
       "             S -> NP S|<ADVP-VP>: 36,\n",
       "             S -> S S|<PAUSE-FRAG>: 18,\n",
       "             S -> NP S|<QUOTE-VP>: 2,\n",
       "             S -> ADVP S|<IN-NP>: 2,\n",
       "             S -> INTJ S|<NP-VP>: 18,\n",
       "             S -> INTJ S|<PAUSE-VP>: 36,\n",
       "             S -> PP S|<BREAK-VP>: 2,\n",
       "             S -> ADVP S|<PAUSE-''>: 4,\n",
       "             S -> S S|<NP-VP>: 18,\n",
       "             S -> SBAR S|<NP-VP>: 6,\n",
       "             S -> NP S|<PAUSE-ADVP>: 4,\n",
       "             S -> PAUSE VP: 6,\n",
       "             S -> RB S|<VP-END>: 4,\n",
       "             S -> VP S|<PAUSE-NP>: 16,\n",
       "             S -> S S|<PAUSE-CC>: 42,\n",
       "             S -> RB S|<NP-VP>: 8,\n",
       "             S -> VP S|<END-QUOTE>: 6,\n",
       "             S -> NP NP: 30,\n",
       "             S -> S END: 4,\n",
       "             S -> S S|<BREAK-FRAG>: 4,\n",
       "             S -> S S|<PAUSE-VP>: 4,\n",
       "             S -> NP S|<BREAK-S>: 2,\n",
       "             S -> MD S|<NP-VP>: 2,\n",
       "             S -> ADVP S|<PAUSE-VP>: 10,\n",
       "             S -> ADJP NP: 4,\n",
       "             S -> ADVP S|<NP-VP>: 8,\n",
       "             S -> S S|<S-S>: 2,\n",
       "             S -> ADVP S|<PP-NP>: 2,\n",
       "             S -> NP S|<NP-TMP-NP>: 2,\n",
       "             S -> PP S|<NP-VP>: 14,\n",
       "             S -> ADVP S|<PAUSE-PP>: 2,\n",
       "             S -> NP-TMP S|<NP-VP>: 4,\n",
       "             S -> S S|<''-CC>: 2,\n",
       "             S -> NP S|<''-VP>: 2,\n",
       "             S -> '' S|<NP-VP>: 2,\n",
       "             S -> ADVP S|<VP-END>: 12,\n",
       "             S -> ADVP S|<PAUSE-S>: 4,\n",
       "             S -> NP S|<VP-VP>: 2,\n",
       "             S -> NP S|<NP-VP>: 8,\n",
       "             S -> SBAR S|<PAUSE-RB>: 2,\n",
       "             S -> ADVP S|<PAUSE-ADVP>: 8,\n",
       "             S -> INTJ S|<VP-END>: 12,\n",
       "             S -> PP S|<PAUSE-VP>: 4,\n",
       "             S -> NP S|<PAUSE-S>: 2,\n",
       "             S -> NP S|<PAUSE-VP>: 10,\n",
       "             S -> S S|<PAUSE-S>: 10,\n",
       "             S -> NP S|<PRN-NP>: 2,\n",
       "             S -> NP PP: 2,\n",
       "             S -> INTJ S|<PAUSE-ADVP>: 10,\n",
       "             S -> CC S|<ADVP-NP>: 2,\n",
       "             S -> NP ADVP: 2,\n",
       "             S -> CC S|<S-CC>: 4,\n",
       "             S -> INTJ S|<PAUSE-CC>: 2,\n",
       "             S -> NP S|<S-,>: 2,\n",
       "             S -> PRN VP: 2,\n",
       "             S -> S S|<BREAK-CC>: 2,\n",
       "             S -> INTJ S|<PAUSE-S>: 6,\n",
       "             S -> PP S|<PAUSE-NP>: 4,\n",
       "             S -> CC S|<SBAR-,>: 4,\n",
       "             S -> CC S|<NP-,>: 8,\n",
       "             S -> CC S|<VP-END>: 2,\n",
       "             S -> ADVP VP: 8,\n",
       "             S -> NP S|<ADJP-ADVP>: 2,\n",
       "             S -> CC S|<PAUSE-S>: 4,\n",
       "             S -> S S|<PAUSE-RB>: 4,\n",
       "             S -> ADVP S|<BREAK-NP>: 2,\n",
       "             S -> CC S|<ADVP-:>: 2,\n",
       "             S -> NP S|<VP-.>: 8,\n",
       "             S -> FRAG S|<NP-VP>: 2,\n",
       "             S -> BREAK S|<NP-VP>: 2,\n",
       "             S -> ADVP S|<PRN-NP>: 2,\n",
       "             S -> NP NP-TMP: 2,\n",
       "             S -> PP S|<BREAK-NP>: 4,\n",
       "             S -> SBAR S|<PAUSE-VP>: 2,\n",
       "             S -> NP S|<VP-:>: 2,\n",
       "             S -> S S: 2,\n",
       "             S -> VP BREAK: 2,\n",
       "             S -> NP S|<VP-PAUSE>: 2,\n",
       "             S -> RB S|<PAUSE-SBAR>: 4,\n",
       "             S -> S S|<QUOTE-VP>: 2,\n",
       "             S -> NP S|<BREAK-VP>: 2,\n",
       "             S -> NP S|<VP-ADVP>: 2,\n",
       "             S -> CC S|<S-,>: 2,\n",
       "             S -> RB S|<PRN-VP>: 4,\n",
       "             S -> ADJP: 2,\n",
       "             S -> RB VP: 2,\n",
       "             S -> NP S|<VP-NP>: 2,\n",
       "             S -> SBAR S|<PAUSE-ADVP>: 2,\n",
       "             S -> PAUSE S|<ADVP-,>: 2,\n",
       "             S -> NP S|<SBAR-END>: 2,\n",
       "             S -> NP S|<ADJP-NP>: 2,\n",
       "             S -> NP S|<PAUSE-PP>: 2,\n",
       "             S -> NP S|<ADJP-S>: 2,\n",
       "             S -> CC S|<PAUSE-SBAR>: 2})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_dict['S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove = {}\n",
    "for k, count in total.items():\n",
    "    for prod, count in s1_dict[k].items():\n",
    "        if \n",
    "        if count < int(total[prod].count * 0.25):\n",
    "            if s in prod.rhs():\n",
    "                remove[s.symbol()] = prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP\n",
      "S|<VP-END>\n"
     ]
    }
   ],
   "source": [
    "for k, c in s1_dict['S'].items():\n",
    "    for i in k.rhs():\n",
    "        print(i.symbol())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"S1_test.gr\", \"w+\") as file:\n",
    "    for k, production in s1_dict.items():\n",
    "        file.write(f\"#### {k} ####\\n\")\n",
    "        for prod, count in sorted(production.items(), key=lambda x: x[1], reverse=True):\n",
    "            if isinstance(prod.rhs()[0], nltk.grammar.Nonterminal):\n",
    "                x = \"{} {} {}\\n\".format(count, prod.lhs(), \" \".join([ str(i) for i in prod.rhs()]))\n",
    "                file.write(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_sentences.txt') as fh:\n",
    "    sents = [line.strip() for line in fh]\n",
    "with open('devset.txt') as fh:\n",
    "    sents += [line.strip() for line in fh]\n",
    "\n",
    "count_dict = dict()\n",
    "count_dict = update_weight(sents, parse_gram, count_dict)\n",
    "rule2gr('S1_updated.gr', 'S2_updated.gr', 'Vocab_updated.gr', count_dict, 'S1_raw.gr', 'S2_raw.gr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
