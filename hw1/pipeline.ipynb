{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pcfg_parse_gen import Pcfg, PcfgGenerator, CkyParse\n",
    "import nltk\n",
    "import benepar\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "from train_grammar import *\n",
    "import re\n",
    "\n",
    "def print_tree(tree_string):\n",
    "    tree_string = tree_string.strip()\n",
    "    tree = nltk.Tree.fromstring(tree_string)\n",
    "    tree.pretty_print()\n",
    "\n",
    "def draw_tree(tree_string):\n",
    "    tree_string = tree_string.strip()\n",
    "    tree = nltk.Tree.fromstring(tree_string)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en to C:\\nltk_data...\n",
      "[nltk_data]   Package benepar_en is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benepar.download('benepar_en')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_parser = benepar.Parser(\"benepar_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allowed_words.txt') as fh:\n",
    "    allowed_words = [line.strip() for line in fh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more scienarios into this function\n",
    "def correct_tag(inputStr):\n",
    "    '''\n",
    "    replace punctuation to avoid cycle, return the new tag\n",
    "    inputStr -- string, string of input tag\n",
    "    '''\n",
    "    outputStr = inputStr.replace(',','Pause')\n",
    "    outputStr = outputStr.replace('.', 'Punc')\n",
    "    return outputStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rules(sents, parser, isS1, allowed_words, exclusion_dict=None,):\n",
    "    '''\n",
    "    find the rules from the trees parsed PennTree parser\n",
    "    sents -- list of strings, a list of sentences\n",
    "    parser -- a nltk parser, benepar in this case\n",
    "    isS1 -- bool, specify if this is for S1 rules\n",
    "    allowed_words -- list of strings, this is used to identify which are grammars and which are vocabs\n",
    "    exclusion_dict -- dictionary, contains the rules we want to exclude in S2\n",
    "                      It is not none when isS1 is False\n",
    "    '''\n",
    "    print(f'================{len(sents)} sentences in total================')\n",
    "    num_finished = 0\n",
    "    trees = [str(parser.parse(sent)) for sent in sents]\n",
    "    count_dict = dict()\n",
    "    isHeadFlag = True\n",
    "    if isS1:\n",
    "        division = 'S1'\n",
    "    else:\n",
    "        division = 'S2'\n",
    "    for tree in trees:\n",
    "        if (num_finished+1) % 100 == 0:\n",
    "            print(f'================{num_finished+1} sentences finished================')\n",
    "        test_tree = Tree.fromstring(tree)\n",
    "        test_tree.chomsky_normal_form(horzMarkov=2)\n",
    "        for subtree in test_tree.subtrees():\n",
    "            if isHeadFlag:\n",
    "                headRule = (division, (subtree.label(), '<Unary>'))\n",
    "                if headRule not in count_dict.keys():\n",
    "                    count_dict[headRule] = 1\n",
    "                    isHeadFlag = False\n",
    "            parent = subtree.label()\n",
    "            if len(subtree) == 2:\n",
    "                try:\n",
    "                    child_left = subtree[0].label()\n",
    "                except:\n",
    "                    child_left = subtree[0]\n",
    "                try:\n",
    "                    child_right = subtree[1].label()\n",
    "                except:\n",
    "                    child_right = subtree[1]\n",
    "            elif len(subtree) == 1:\n",
    "                try:\n",
    "                    child_left = subtree[0].label()\n",
    "                except:\n",
    "                    child_left = subtree[0]\n",
    "                child_right = '<Unary>'\n",
    "            else:\n",
    "                continue\n",
    "            # if this should belong to vocab, skip it\n",
    "            if child_left in allowed_words:\n",
    "                continue\n",
    "            this_rule = (parent, (child_left, child_right))\n",
    "            if not isS1:\n",
    "                if this_rule not in exclusion_dict.keys():\n",
    "                    if this_rule not in count_dict.keys():\n",
    "                        count_dict[this_rule] = 1\n",
    "            else:\n",
    "                count_dict[this_rule] = 1\n",
    "            #print(parse_gram.rules[i][2])\n",
    "        isHeadFlag = True\n",
    "        num_finished += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================27 sentences in total================\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('example_sentences.txt') as fh:\n",
    "    sents = [line.strip() for line in fh]\n",
    "s1_dict = find_rules(sents, bst_parser, True, allowed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================500 sentences in total================\n",
      "================100 sentences finished================\n",
      "================200 sentences finished================\n",
      "================300 sentences finished================\n",
      "================400 sentences finished================\n",
      "================500 sentences finished================\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('devset.txt') as fh:\n",
    "    sents = [line.strip() for line in fh]\n",
    "s2_dict = find_rules(sents, bst_parser, False, allowed_words, s1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_rules(OutputFileName, count_dict):\n",
    "    '''\n",
    "    store the count_dict locally with correct format\n",
    "    OutputFileName -- string, the name of the output file\n",
    "    count_dict -- dictionary, contains the updated count\n",
    "    '''    \n",
    "    f_output = open(OutputFileName, \"w\")\n",
    "    for key, value in count_dict.items():\n",
    "            if key[1][1] == '<Unary>':\n",
    "                f_output.write(f'{value}\\t{key[0]}\\t{key[1][0]}\\n')\n",
    "            else:\n",
    "                f_output.write(f'{value}\\t{key[0]}\\t{key[1][0]} {key[1][1]}\\n')\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rules('S1_raw.gr', s1_dict)\n",
    "output_rules('S2_raw.gr', s2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#reading grammar file: S1_raw.gr\n",
      "#reading grammar file: S2_raw.gr\n",
      "#Ignored cycle `` -> ``\n",
      "#Ignored cycle NP -> NP\n",
      "#Ignored cycle NP -> NP\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: unexpected line at line 910: 1 |<.->",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a52c5f50367b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#parse_gram = Pcfg([\"S1_yabing.gr\",\"S2_yabing.gr\",\"Vocab_yabing.gr\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparse_gram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPcfg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"S1_raw.gr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"S2_raw.gr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tagged_allowed_words.txt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\nlp\\nlpclass-1187-g-Takemeflytoo\\hw1\\pcfg_parse_gen.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filelist, startsym, allowed_words_file, verbose)\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[1;31m# empty rules not allowed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     raise ValueError(\"Error: unexpected line at line %d: %s\"\n\u001b[1;32m--> 104\u001b[1;33m                                      % (linenum, ' '.join(f)))\n\u001b[0m\u001b[0;32m    105\u001b[0m                 \u001b[1;31m# count lhs left [right]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error: unexpected line at line 910: 1 |<.->"
     ]
    }
   ],
   "source": [
    "#parse_gram = Pcfg([\"S1_yabing.gr\",\"S2_yabing.gr\",\"Vocab_yabing.gr\"])\n",
    "parse_gram = Pcfg([\"S1_raw.gr\",\"S2_raw.gr\",\"tagged_allowed_words.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(sents, parse_gram, count_dict):\n",
    "    '''\n",
    "    update the weight recorded in count_dict, return the updated count_dict\n",
    "    sents -- list of strings, a list of sentences\n",
    "    parse_gram -- Pcfg instance\n",
    "    count_dict -- dictionary, contains the old count\n",
    "    '''\n",
    "    parser = CkyParse(parse_gram, beamsize=0.0001)\n",
    "    ce, trees = parser.parse_sentences(sents)\n",
    "    for tree in trees:\n",
    "        test_tree = Tree.fromstring(tree)\n",
    "        for subtree in test_tree.subtrees():\n",
    "            parent = subtree.label()\n",
    "            if len(subtree) == 2:\n",
    "                try:\n",
    "                    child_left = subtree[0].label()\n",
    "                except:\n",
    "                    child_left = subtree[0]\n",
    "                try:\n",
    "                    child_right = subtree[1].label()\n",
    "                except:\n",
    "                    child_right = subtree[1]\n",
    "            elif len(subtree) == 1:\n",
    "                try:\n",
    "                    child_left = subtree[0].label()\n",
    "                except:\n",
    "                    child_left = subtree[0]\n",
    "                child_right = '<Unary>'\n",
    "            else:\n",
    "                continue\n",
    "            for i in parse_gram.rhs[(child_left, child_right)]:\n",
    "                if parse_gram.rules[i][0] == parent:\n",
    "                    this_rule = (parse_gram.rules[i][0], parse_gram.rules[i][1])\n",
    "                    if this_rule in count_dict.keys():\n",
    "                        count_dict[this_rule] += 1\n",
    "                    else:\n",
    "                        count_dict[this_rule] = 2\n",
    "                    #print(parse_gram.rules[i][2])\n",
    "    # if the original rules were not utilized, add them to count_dict\n",
    "    for key, value in parse_gram.rules.items():\n",
    "        this_rule = (value[0], value[1])\n",
    "        if this_rule not in count_dict.keys():\n",
    "            count_dict[this_rule] = value[2]\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule2gr(s1OutputFileName, s2OutputFileName, vocabOutputFileName, count_dict, s1InputFileName, s2InputFileName):\n",
    "    '''\n",
    "    store the count_dict locally with correct format\n",
    "    s1OutputFileName -- string, the name of the output file for S1\n",
    "    s2OutputFileName -- string, the name of the output file for S2\n",
    "    vocabOutputFileName -- string, the name of the output file for Vocab\n",
    "    count_dict -- dictionary, contains the updated count\n",
    "    s1InputFileName -- string, the filename of the original S1\n",
    "    s2InputFileName -- string, the filename of the original S2\n",
    "    '''\n",
    "    s1_keys = list()\n",
    "    s2_keys = list()\n",
    "    f_s1 = open(s1InputFileName, \"r\")\n",
    "    for line in f_s1:\n",
    "        line = line.strip()\n",
    "        # skip the comment line\n",
    "        if line and (not line.startswith(\"#\")):\n",
    "            # split the line and extract count, pos_tag, and terminal\n",
    "            line_arr = re.split('[\\s]+',line)\n",
    "            if len(line_arr) == 4:\n",
    "                s1_key = (line_arr[1], (line_arr[2], line_arr[3]))\n",
    "            else:\n",
    "                s1_key = (line_arr[1], (line_arr[2], '<Unary>'))\n",
    "            if s1_key not in s1_keys: \n",
    "                s1_keys.append(s1_key)\n",
    "    f_s1.close()\n",
    "    \n",
    "    f_s2 = open(s2InputFileName, \"r\")\n",
    "    for line in f_s2:\n",
    "        line = line.strip()\n",
    "        # skip the comment line\n",
    "        if line and (not line.startswith(\"#\")):\n",
    "            # split the line and extract count, pos_tag, and terminal\n",
    "            line_arr = re.split('[\\s]+',line)\n",
    "            if len(line_arr) == 4:\n",
    "                s2_key = (line_arr[1], (line_arr[2], line_arr[3]))\n",
    "            else:\n",
    "                s2_key = (line_arr[1], (line_arr[2], '<Unary>'))\n",
    "            if s2_key not in s2_keys: \n",
    "                s2_keys.append(s2_key)\n",
    "    f_s2.close()\n",
    "    \n",
    "    f_s1_output = open(s1OutputFileName, \"w\")\n",
    "    f_s2_output = open(s2OutputFileName, \"w\")\n",
    "    f_vocab_output = open(vocabOutputFileName, \"w\")\n",
    "    isVocabFlag = True\n",
    "    for key, value in count_dict.items():\n",
    "        if key in s1_keys:\n",
    "            if key[1][1] == '<Unary>':\n",
    "                f_s1_output.write(f'{value}\\t{key[0]}\\t{key[1][0]}\\n')\n",
    "            else:\n",
    "                f_s1_output.write(f'{value}\\t{key[0]}\\t{key[1][0]} {key[1][1]}\\n')\n",
    "            isVocabFlag = False\n",
    "        if key in s2_keys:\n",
    "            if key[1][1] == '<Unary>':\n",
    "                f_s2_output.write(f'{value}\\t{key[0]}\\t{key[1][0]}\\n')\n",
    "            else:\n",
    "                f_s2_output.write(f'{value}\\t{key[0]}\\t{key[1][0]} {key[1][1]}\\n')\n",
    "            isVocabFlag = False\n",
    "        if isVocabFlag:\n",
    "            if key[1][1] == '<Unary>':\n",
    "                f_vocab_output.write(f'{value}\\t{key[0]}\\t{key[1][0]}\\n')\n",
    "            else:\n",
    "                f_vocab_output.write(f'{value}\\t{key[0]}\\t{key[1][0]} {key[1][1]}\\n')\n",
    "        isVocabFlag = True\n",
    "    f_s1_output.close()\n",
    "    f_s2_output.close()\n",
    "    f_vocab_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_sentences.txt') as fh:\n",
    "    sents = [line.strip() for line in fh]\n",
    "with open('devset.txt') as fh:\n",
    "    sents += [line.strip() for line in fh]\n",
    "\n",
    "count_dict = dict()\n",
    "count_dict = update_weight(sents, parse_gram, count_dict)\n",
    "rule2gr('S1_updated.gr', 'S2_updated.gr', 'Vocab_updated.gr', count_dict, 'S1_raw.gr', 'S2_raw.gr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
