{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 Aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LLR Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the package and dataset <a id='part-0'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optparse, sys, os, logging\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts_datadir, opts_fileprefix = \"data\", \"hansards\"\n",
    "opts_french, opts_english = \"fr\", \"en\"\n",
    "# opts_datadir, opts_fileprefix = \"data\", \"europarl\"\n",
    "# opts_french, opts_english = \"de\", \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts_num_sents = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data = \"%s.%s\" % (os.path.join(opts_datadir, opts_fileprefix), opts_french)\n",
    "e_data = \"%s.%s\" % (os.path.join(opts_datadir, opts_fileprefix), opts_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitext = [[sentence.strip().split() for sentence in pair] \\\n",
    "          for pair in islice(zip(open(f_data,encoding=\"utf8\"), open(e_data,encoding=\"utf8\")), opts_num_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage: theta = LLR_initialization(bitext)\n",
    "def LLR_initialization(bitext, exp = 1, isReverse=False): \n",
    "    if isReverse:\n",
    "        f=1\n",
    "        e=0\n",
    "    else:\n",
    "        f=0\n",
    "        e=1\n",
    "    LLRs = defaultdict(float)\n",
    "    sum_LLRs = defaultdict(float)\n",
    "    \n",
    "    t = set()\n",
    "    s = set()\n",
    "    \n",
    "    t_count = defaultdict(int) # occurance of each words in t\n",
    "    s_count = defaultdict(int) # occurance of each words in s\n",
    "    ts_count = defaultdict(int) # occurance of each (t, s) pair\n",
    "    \n",
    "    for pair in bitext:\n",
    "        t = t.union(set(pair[f]))\n",
    "        s = s.union(set(pair[e]))\n",
    "        \n",
    "        for t_i in set(pair[f]):\n",
    "            t_count[t_i] += 1\n",
    "            for s_i in set(pair[e]):\n",
    "                ts_count[(t_i, s_i)] += 1\n",
    "            \n",
    "        for s_i in set(pair[e]):\n",
    "            s_count[s_i] += 1\n",
    "      \n",
    "    sum_ts_count = sum(ts_count.values())\n",
    "    sum_t_count = sum(t_count.values())\n",
    "    sum_s_count = sum(s_count.values())\n",
    "\n",
    "    for (t_j, s_j) in ts_count.keys():\n",
    "        p_ts = ts_count[(t_j, s_j)] / sum_ts_count\n",
    "        p_t = t_count[t_j] / sum_t_count\n",
    "        p_s = s_count[s_j] / sum_s_count\n",
    "        \n",
    "        if p_ts > p_t * p_s:\n",
    "            # calculate LLR\n",
    "            \n",
    "            # (t? == t) and (s? == s)\n",
    "            count_for_LLR_1 = ts_count[(t_j, s_j)]\n",
    "            if count_for_LLR_1 != 0:\n",
    "                LLR_1 = count_for_LLR_1 * math.log10((p_ts/p_s)/p_t)\n",
    "            \n",
    "            # (t? == t) and (s? == not s)\n",
    "            count_for_LLR_2 = (t_count[t_j] - ts_count[(t_j, s_j)])\n",
    "            if count_for_LLR_2 != 0:\n",
    "                LLR_2 = count_for_LLR_2 * math.log10(((count_for_LLR_2 / sum_ts_count)/(1-p_s))/p_t)\n",
    "            \n",
    "            # (t? == not t) and (s? == s)\n",
    "            count_for_LLR_3 = (s_count[s_j] - ts_count[(t_j, s_j)]) \n",
    "            if count_for_LLR_3 != 0:\n",
    "                LLR_3 = count_for_LLR_3 * math.log10(((count_for_LLR_3 / sum_ts_count)/p_s)/(1-p_t))\n",
    "            \n",
    "            # (t? == not t) and (s? == not s)\n",
    "            count_for_LLR_4 = sum_ts_count - count_for_LLR_1 - count_for_LLR_2 - count_for_LLR_3\n",
    "            if count_for_LLR_4 != 0:\n",
    "                LLR_4 = count_for_LLR_4 * math.log10(((count_for_LLR_4 / sum_ts_count)/(1-p_s))/(1-p_t))\n",
    "            \n",
    "            LLRs[(t_j, s_j)] = LLR_1 + LLR_2 + LLR_3 + LLR_4\n",
    "        else:\n",
    "            # p(t, s) <= p(t) * p(s), so initialize to uniform distribution\n",
    "            LLRs[(t_j, s_j)] = 1.0 / len(t) \n",
    "            \n",
    "        # for each source word, compute the sum of the LLR scores over all target words\n",
    "        sum_LLRs[s_j] += LLRs[(t_j, s_j)]\n",
    "            \n",
    "       \n",
    "    \n",
    "    # then divide every LLR score by the single largest of these sums\n",
    "                  \n",
    "    largest = max(sum_LLRs.values()) \n",
    "\n",
    "    for i in LLRs.keys():\n",
    "        LLRs[i] = LLRs[i] / largest\n",
    "        \n",
    "    # raise each LLR score to an empirically optimized exponent\n",
    "    for i in LLRs.keys():\n",
    "        LLRs[i] = LLRs[i] ** exp\n",
    "               \n",
    "    return LLRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very beginning, I implemented the baseline according to the assignment description provided by the professor. It achieved 0.3417 of AER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with Expectation Maximization...\n"
     ]
    }
   ],
   "source": [
    "sys.stderr.write(\"Training with Expectation Maximization...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 0 ns, total: 1min 48s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# f is the French word set\n",
    "# e is the English word set\n",
    "# f_count is the word count dictionary for French word set\n",
    "# N is the number of sentences\n",
    "f = set()\n",
    "e = set()\n",
    "f_count = defaultdict(int)\n",
    "for pair in bitext:\n",
    "    f = f.union(set(pair[0]))\n",
    "    e = e.union(set(pair[1]))\n",
    "    for f_i in set(pair[0]):\n",
    "        f_count[f_i] += 1\n",
    "N = len(bitext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $k = 0$<br>\n",
    "* Initialize $t_0$ **## Easy choice: initialize uniformly ##**<br>\n",
    "* repeat <br>\n",
    "    * $k$ += 1 <br>\n",
    "    * Initialize all counts to zero <br>\n",
    "    * for each $(\\textbf{f}, \\textbf{e})$ in ${\\cal D}$ <br>\n",
    "        * for each $f_i$ in $\\textbf{f}$ <br>\n",
    "            * $Z$ = 0 **## Z commonly denotes a normalization term ##** <br>\n",
    "            * for each $e_j$ in $\\textbf{e}$ <br>\n",
    "                * $Z$ += $t_{k-1}(f_i \\mid e_j)$ <br>\n",
    "            * for each $e_j$ in $\\textbf{e}$ <br>\n",
    "                * `c` = $ t_{k-1}(f_i \\mid e_j) / Z $ <br>\n",
    "                * count($f_i$, $e_j$) += `c` <br>\n",
    "                * count($e_j$) += `c` <br>\n",
    "    * for each ($f$, $e$) in count <br>\n",
    "        * Set new parameters: $t_k(f \\mid e)$ =  count($f,e$) / count($e$) <br>\n",
    "* until k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1.................................\n",
      "Iteration 1 finished. Time cost: 86.66741967201233\n",
      "Iteration 2.................................\n",
      "Iteration 2 finished. Time cost: 87.8349084854126\n",
      "Iteration 3.................................\n",
      "Iteration 3 finished. Time cost: 87.54906606674194\n",
      "Iteration 4.................................\n",
      "Iteration 4 finished. Time cost: 87.58197975158691\n",
      "Iteration 5.................................\n",
      "Iteration 5 finished. Time cost: 88.14286184310913\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "# initialize theta uniformly\n",
    "num_f = len(f_count)\n",
    "theta = LLR_initialization(bitext)# defaultdict(lambda: 1./num_f)\n",
    "while k < 5:\n",
    "    k += 1\n",
    "    tic = time.time()\n",
    "    sys.stderr.write(f\"Iteration {k}.................................\\n\")\n",
    "    e_count = defaultdict(int)\n",
    "    fe_count = defaultdict(int)\n",
    "    for n in range(N):\n",
    "        for f_i in bitext[n][0]:\n",
    "            Z = 0\n",
    "            for e_j in bitext[n][1]:\n",
    "                Z += theta[(f_i, e_j)]\n",
    "            for e_j in bitext[n][1]:\n",
    "                c = theta[(f_i, e_j)] / Z\n",
    "                fe_count[(f_i, e_j)] += c\n",
    "                e_count[e_j] += c\n",
    "    for (f_i, e_j) in fe_count.keys():\n",
    "        theta[(f_i, e_j)] = fe_count[(f_i, e_j)] / e_count[e_j]\n",
    "    toc = time.time()\n",
    "    sys.stderr.write(f\"Iteration {k} finished. Time cost: {toc-tic}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for each $(\\textbf{f}, \\textbf{e})$ in ${\\cal D}$\n",
    "    * for each $f_i$ in $\\textbf{f}$\n",
    "        * `bestp` = 0\n",
    "        * `bestj` = 0\n",
    "        * for each $e_j$ in $\\textbf{e}$\n",
    "            * if $t(f_i \\mid e_j)$ > `bestp`\n",
    "                * `bestp` = $t(f_i \\mid e_j)$\n",
    "                * `bestj` = $j$\n",
    "        * align $f_i$ to $e_{\\texttt{bestj}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning...\n"
     ]
    }
   ],
   "source": [
    "sys.stderr.write(\"Aligning...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr dice_a\n",
    "for f, e in bitext:\n",
    "    for i in range(len(f)):\n",
    "        f_i = f[i]\n",
    "        bestp = 0\n",
    "        bestj = 0\n",
    "        for j in range(len(e)):\n",
    "            e_j = e[j]\n",
    "            if theta[(f_i, e_j)] > bestp:\n",
    "                bestp = theta[(f_i, e_j)]\n",
    "                bestj = j\n",
    "        sys.stdout.write(f\"{i}-{bestj} \")\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the output to the local file dice.a\n",
    "with open('dice.a','w') as fh:\n",
    "    fh.write(str(dice_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run check-alignments.py -i dice.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.553022\n",
      "Recall = 0.702576\n",
      "AER = 0.395796\n"
     ]
    }
   ],
   "source": [
    "%run score-alignments.py -n 0 -i dice.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add n smoothing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this trial, I added n smoothing to the baseline. The AER was improved from 0.34 to 0.3124. The value of n is set to be 0.01. I didn't see much difference it can make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 15.6 ms, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# f is the French word set\n",
    "# e is the English word set\n",
    "# f_count is the word count dictionary for French word set\n",
    "# N is the number of sentences\n",
    "f = set()\n",
    "e = set()\n",
    "f_count = defaultdict(int)\n",
    "for pair in bitext:\n",
    "    f = f.union(set(pair[0]))\n",
    "    e = e.union(set(pair[1]))\n",
    "    for f_i in set(pair[0]):\n",
    "        f_count[f_i] += 1\n",
    "N = len(bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add n smoothing\n",
    "smooth_n = 0.01\n",
    "vocab_N = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1.................................\n",
      "Iteration 1 finished. Time cost: 87.42067885398865\n",
      "Iteration 2.................................\n",
      "Iteration 2 finished. Time cost: 87.03539776802063\n",
      "Iteration 3.................................\n",
      "Iteration 3 finished. Time cost: 87.75008034706116\n",
      "Iteration 4.................................\n",
      "Iteration 4 finished. Time cost: 86.99113941192627\n",
      "Iteration 5.................................\n",
      "Iteration 5 finished. Time cost: 87.16883683204651\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "# initialize theta uniformly\n",
    "num_f = len(f_count)\n",
    "theta = LLR_initialization(bitext)#defaultdict(lambda: 1./num_f)\n",
    "while k < 5:\n",
    "    k += 1\n",
    "    tic = time.time()\n",
    "    sys.stderr.write(f\"Iteration {k}.................................\\n\")\n",
    "    e_count = defaultdict(int)\n",
    "    fe_count = defaultdict(int)\n",
    "    for n in range(N):\n",
    "        for f_i in bitext[n][0]:\n",
    "            Z = 0\n",
    "            for e_j in bitext[n][1]:\n",
    "                Z += theta[(f_i, e_j)]\n",
    "            for e_j in bitext[n][1]:\n",
    "                c = theta[(f_i, e_j)] / Z\n",
    "                fe_count[(f_i, e_j)] += c\n",
    "                e_count[e_j] += c\n",
    "    for (f_i, e_j) in fe_count.keys():\n",
    "        theta[(f_i, e_j)] = (fe_count[(f_i, e_j)] + smooth_n) / (e_count[e_j] + vocab_N * smooth_n)\n",
    "    toc = time.time()\n",
    "    sys.stderr.write(f\"Iteration {k} finished. Time cost: {toc-tic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr dice_a\n",
    "for f, e in bitext:\n",
    "    for i in range(len(f)):\n",
    "        f_i = f[i]\n",
    "        bestp = 0\n",
    "        bestj = 0\n",
    "        for j in range(len(e)):\n",
    "            e_j = e[j]\n",
    "            if theta[(f_i, e_j)] > bestp:\n",
    "                bestp = theta[(f_i, e_j)]\n",
    "                bestj = j\n",
    "        sys.stdout.write(f\"{i}-{bestj} \")\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the output to the local file dice.a\n",
    "with open('dice.a','w',encoding='utf8') as fh:\n",
    "    fh.write(str(dice_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.622858\n",
      "Recall = 0.808073\n",
      "AER = 0.313755\n"
     ]
    }
   ],
   "source": [
    "%run score-alignments.py -n 0 -i dice.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use posterior probabilities + add n smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this trial, I used posterior probabilities intead of argmax method. Depending on the value of the threshold, the precision and recall can be differed. There is always a tradeoff between precision and recall while adjusting the threshold. I indeed saw an improvement of AER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 0 ns, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# f is the French word set\n",
    "# e is the English word set\n",
    "# f_count is the word count dictionary for French word set\n",
    "# N is the number of sentences\n",
    "f = set()\n",
    "e = set()\n",
    "f_count = defaultdict(int)\n",
    "for pair in bitext:\n",
    "    f = f.union(set(pair[0]))\n",
    "    e = e.union(set(pair[1]))\n",
    "    for f_i in set(pair[0]):\n",
    "        f_count[f_i] += 1\n",
    "N = len(bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add n smoothing\n",
    "smooth_n = 0.01\n",
    "vocab_N = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1.................................\n",
      "Iteration 1 finished. Time cost: 87.26017236709595\n",
      "Iteration 2.................................\n",
      "Iteration 2 finished. Time cost: 87.55622863769531\n",
      "Iteration 3.................................\n",
      "Iteration 3 finished. Time cost: 87.71975445747375\n",
      "Iteration 4.................................\n",
      "Iteration 4 finished. Time cost: 87.12346315383911\n",
      "Iteration 5.................................\n",
      "Iteration 5 finished. Time cost: 87.53006911277771\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "# initialize theta uniformly\n",
    "num_f = len(f_count)\n",
    "theta = LLR_initialization(bitext)#defaultdict(lambda: 1./num_f)\n",
    "while k < 5:\n",
    "    k += 1\n",
    "    tic = time.time()\n",
    "    sys.stderr.write(f\"Iteration {k}.................................\\n\")\n",
    "    e_count = defaultdict(int)\n",
    "    fe_count = defaultdict(int)\n",
    "    for n in range(N):\n",
    "        for f_i in bitext[n][0]:\n",
    "            Z = 0\n",
    "            for e_j in bitext[n][1]:\n",
    "                Z += theta[(f_i, e_j)]\n",
    "            for e_j in bitext[n][1]:\n",
    "                c = theta[(f_i, e_j)] / Z\n",
    "                fe_count[(f_i, e_j)] += c\n",
    "                e_count[e_j] += c\n",
    "    for (f_i, e_j) in fe_count.keys():\n",
    "        theta[(f_i, e_j)] = (fe_count[(f_i, e_j)] + smooth_n) / (e_count[e_j] + vocab_N * smooth_n)\n",
    "    toc = time.time()\n",
    "    sys.stderr.write(f\"Iteration {k} finished. Time cost: {toc-tic}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for each $(\\textbf{f},\\textbf{e})$ in ${\\cal D}$\n",
    "    * for each $f_i$ in $\\textbf{f}$\n",
    "        * $Z = 0$\n",
    "        * for each $e_j$ in $\\textbf{e}$\n",
    "            * $Z += t(f_i∣e_j)$\n",
    "        * for each $e_j$ in $\\textbf{e}$\n",
    "            * $posterior$ = $t(f_i∣e_j)/ Z$\n",
    "            * if $(posterior > \\delta)$ keep alignment between $f_i$ and $e_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr dice_a\n",
    "\n",
    "# delta is the threshold we used to decide whether to keep the alignment pair\n",
    "delta = 0.3\n",
    "\n",
    "for f, e in bitext:\n",
    "    for i, f_i in enumerate(f):\n",
    "        Z = 0\n",
    "        for j, e_j in enumerate(e):\n",
    "            Z += theta[(f_i, e_j)]\n",
    "        for j, e_j in enumerate(e):\n",
    "            posterior = theta[(f_i, e_j)] / Z\n",
    "            if posterior >= delta:\n",
    "                sys.stdout.write(f\"{i}-{j} \")\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the output to the local file dice.a\n",
    "with open('dice.a','w',encoding='utf8') as fh:\n",
    "    fh.write(str(dice_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.634238\n",
      "Recall = 0.832343\n",
      "AER = 0.297843\n"
     ]
    }
   ],
   "source": [
    "%run score-alignments.py -n 0 -i dice.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection of alignments from two directions + add n smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this trial, I tried to build two models from both translation directions and used the intersection of two alignment sets as final alignment. The AER was improved dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 15.6 ms, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# f is the French word set\n",
    "# e is the English word set\n",
    "# f_count is the word count dictionary for French word set\n",
    "# e_count is the word count dictionary for English word set\n",
    "# N is the number of sentences\n",
    "f = set()\n",
    "e = set()\n",
    "f_count = defaultdict(int)\n",
    "e_count = defaultdict(int)\n",
    "for pair in bitext:\n",
    "    f = f.union(set(pair[0]))\n",
    "    e = e.union(set(pair[1]))\n",
    "    for f_i in set(pair[0]):\n",
    "        f_count[f_i] += 1\n",
    "    for e_j in set(pair[1]):\n",
    "        e_count[e_j] += 1\n",
    "N = len(bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_f = len(f_count)\n",
    "num_e = len(e_count)\n",
    "\n",
    "# add n smoothing\n",
    "smooth_n = 0.01\n",
    "vocab_N = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(num, N, isReverse=False):\n",
    "    '''\n",
    "    num: size of target language word count dict\n",
    "    N: number of sentences\n",
    "    isReverse: if the translation direction is reversed\n",
    "    '''\n",
    "    k = 0\n",
    "    # initialize theta uniformly\n",
    "    theta = LLR_initialization(bitext,1,isReverse)#defaultdict(lambda: 1./num)\n",
    "    while k < 5:\n",
    "        k += 1\n",
    "        tic = time.time()\n",
    "        sys.stderr.write(f\"Iteration {k}.................................\\n\")\n",
    "        e_count = defaultdict(int)\n",
    "        fe_count = defaultdict(int)\n",
    "        if isReverse:\n",
    "            for n in range(N):\n",
    "                for f_i in bitext[n][1]:\n",
    "                    Z = 0\n",
    "                    for e_j in bitext[n][0]:\n",
    "                        Z += theta[(f_i, e_j)]\n",
    "                    for e_j in bitext[n][0]:\n",
    "                        c = theta[(f_i, e_j)] / Z\n",
    "                        fe_count[(f_i, e_j)] += c\n",
    "                        e_count[e_j] += c\n",
    "        else:\n",
    "            for n in range(N):\n",
    "                for f_i in bitext[n][0]:\n",
    "                    Z = 0\n",
    "                    for e_j in bitext[n][1]:\n",
    "                        Z += theta[(f_i, e_j)]\n",
    "                    for e_j in bitext[n][1]:\n",
    "                        c = theta[(f_i, e_j)] / Z\n",
    "                        fe_count[(f_i, e_j)] += c\n",
    "                        e_count[e_j] += c\n",
    "        for (f_i, e_j) in fe_count.keys():\n",
    "            theta[(f_i, e_j)] = (fe_count[(f_i, e_j)] + smooth_n) / (e_count[e_j] + vocab_N * smooth_n)\n",
    "        toc = time.time()\n",
    "        sys.stderr.write(f\"Iteration {k} finished. Time cost: {toc-tic}\\n\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1.................................\n",
      "Iteration 1 finished. Time cost: 70.76888418197632\n",
      "Iteration 2.................................\n",
      "Iteration 2 finished. Time cost: 71.99081039428711\n",
      "Iteration 3.................................\n",
      "Iteration 3 finished. Time cost: 72.3838472366333\n",
      "Iteration 4.................................\n",
      "Iteration 4 finished. Time cost: 71.63812279701233\n",
      "Iteration 5.................................\n",
      "Iteration 5 finished. Time cost: 71.80967497825623\n",
      "Iteration 1.................................\n",
      "Iteration 1 finished. Time cost: 68.51897931098938\n",
      "Iteration 2.................................\n",
      "Iteration 2 finished. Time cost: 69.1212432384491\n",
      "Iteration 3.................................\n",
      "Iteration 3 finished. Time cost: 68.7495653629303\n",
      "Iteration 4.................................\n",
      "Iteration 4 finished. Time cost: 68.99606847763062\n",
      "Iteration 5.................................\n",
      "Iteration 5 finished. Time cost: 69.08950090408325\n"
     ]
    }
   ],
   "source": [
    "theta_e2f = align(num_f, N, False)\n",
    "theta_f2e = align(num_e, N, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr dice_a\n",
    "for f, e in bitext:\n",
    "    set_e2f = set()\n",
    "    set_f2e = set()\n",
    "    for i in range(len(f)):\n",
    "        f_i = f[i]\n",
    "        bestp = 0\n",
    "        bestj = 0\n",
    "        for j in range(len(e)):\n",
    "            e_j = e[j]\n",
    "            if theta_e2f[(f_i, e_j)] > bestp:\n",
    "                bestp = theta_e2f[(f_i, e_j)]\n",
    "                bestj = j\n",
    "        set_e2f.add((i, bestj))        \n",
    "    for j in range(len(e)):\n",
    "        e_j = e[j]\n",
    "        bestp = 0\n",
    "        besti = 0\n",
    "        for i in range(len(f)):\n",
    "            f_i = f[i]\n",
    "            if theta_f2e[(e_j, f_i)] > bestp:\n",
    "                bestp = theta_f2e[(e_j, f_i)]\n",
    "                besti = i\n",
    "        set_f2e.add((besti,j))\n",
    "    set_combined = set_f2e.intersection(set_e2f)\n",
    "    \n",
    "    for pair in set_combined:\n",
    "        sys.stdout.write(f\"{pair[0]}-{pair[1]} \")\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the output to the local file dice.a\n",
    "with open('dice.a','w',encoding='utf8') as fh:\n",
    "    fh.write(str(dice_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.845532\n",
      "Recall = 0.724121\n",
      "AER = 0.213048\n"
     ]
    }
   ],
   "source": [
    "%run score-alignments.py -n 0 -i dice.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment by agreement + add n smoothing + posterier probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this trial, I combined everything I've tried, including adding n smoothing, applying posterior probabilities while aligning, and applying alignment by two independent models. I experimented several thresholds (see below). As you may have noted, the threshold is almost the square of the threshold used in previous trial. The reason is we are thresholding the product of two posterior probabilities. Eventually, the optimized one was determined to be 0.08. I also tried different n (in add n smoothing) (results not shown). That won't make much difference on AER. So I simply set it to 0.01 as specified in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    n = 0.01 \n",
    "    Threshold | AER score | Precision | Recall\n",
    "    0.04      | 0.215656  | 0.747634  | 0.841010\n",
    "    0.06      | 0.197782  | 0.800442  | 0.804606\n",
    "    0.07      | 0.194017  | 0.817271  | 0.791481\n",
    "    0.08      | 0.192571  | 0.829273  | 0.780337\n",
    "    0.09      | 0.194620  | 0.836773  | 0.767707\n",
    "    0.10      | 0.196451  | 0.844369  | 0.756067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 0 ns, total: 1min 48s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# f is the French word set\n",
    "# e is the English word set\n",
    "# f_count is the word count dictionary for French word set\n",
    "# e_count is the word count dictionary for English word set\n",
    "# N is the number of sentences\n",
    "f = set()\n",
    "e = set()\n",
    "f_count = defaultdict(int)\n",
    "e_count = defaultdict(int)\n",
    "for pair in bitext:\n",
    "    f = f.union(set(pair[0]))\n",
    "    e = e.union(set(pair[1]))\n",
    "    for f_i in set(pair[0]):\n",
    "        f_count[f_i] += 1\n",
    "    for e_j in set(pair[1]):\n",
    "        e_count[e_j] += 1\n",
    "N = len(bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_f = len(f_count)\n",
    "num_e = len(e_count)\n",
    "\n",
    "# add n smoothing\n",
    "smooth_n = 0.01\n",
    "vocab_N = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(num, N, isReverse=False):\n",
    "    '''\n",
    "    num: size of target language word count dict\n",
    "    N: number of sentences\n",
    "    isReverse: if the translation direction is reversed\n",
    "    '''\n",
    "    k = 0\n",
    "    # initialize theta uniformly\n",
    "    theta = LLR_initialization(bitext,1,isReverse)#defaultdict(lambda: 1./num)\n",
    "    while k < 5:\n",
    "        k += 1\n",
    "        tic = time.time()\n",
    "        sys.stderr.write(f\"Iteration {k}.................................\\n\")\n",
    "        e_count = defaultdict(int)\n",
    "        fe_count = defaultdict(int)\n",
    "        if isReverse:\n",
    "            for n in range(N):\n",
    "                for f_i in bitext[n][1]:\n",
    "                    Z = 0\n",
    "                    for e_j in bitext[n][0]:\n",
    "                        Z += theta[(f_i, e_j)]\n",
    "                    for e_j in bitext[n][0]:\n",
    "                        c = theta[(f_i, e_j)] / Z\n",
    "                        fe_count[(f_i, e_j)] += c\n",
    "                        e_count[e_j] += c\n",
    "        else:\n",
    "            for n in range(N):\n",
    "                for f_i in bitext[n][0]:\n",
    "                    Z = 0\n",
    "                    for e_j in bitext[n][1]:\n",
    "                        Z += theta[(f_i, e_j)]\n",
    "                    for e_j in bitext[n][1]:\n",
    "                        c = theta[(f_i, e_j)] / Z\n",
    "                        fe_count[(f_i, e_j)] += c\n",
    "                        e_count[e_j] += c\n",
    "        for (f_i, e_j) in fe_count.keys():\n",
    "            theta[(f_i, e_j)] = (fe_count[(f_i, e_j)] + smooth_n) / (e_count[e_j] + vocab_N * smooth_n)\n",
    "        toc = time.time()\n",
    "        sys.stderr.write(f\"Iteration {k} finished. Time cost: {toc-tic}\\n\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1.................................\n",
      "Iteration 1 finished. Time cost: 69.84036207199097\n",
      "Iteration 2.................................\n",
      "Iteration 2 finished. Time cost: 70.30485582351685\n",
      "Iteration 3.................................\n",
      "Iteration 3 finished. Time cost: 70.22057962417603\n",
      "Iteration 4.................................\n",
      "Iteration 4 finished. Time cost: 70.36179447174072\n",
      "Iteration 5.................................\n",
      "Iteration 5 finished. Time cost: 70.32945227622986\n",
      "Iteration 1.................................\n",
      "Iteration 1 finished. Time cost: 69.34241795539856\n",
      "Iteration 2.................................\n",
      "Iteration 2 finished. Time cost: 69.60418105125427\n",
      "Iteration 3.................................\n",
      "Iteration 3 finished. Time cost: 69.29814672470093\n",
      "Iteration 4.................................\n",
      "Iteration 4 finished. Time cost: 69.81924319267273\n",
      "Iteration 5.................................\n",
      "Iteration 5 finished. Time cost: 69.46430253982544\n"
     ]
    }
   ],
   "source": [
    "theta_e2f = align(num_f, N, False)\n",
    "theta_f2e = align(num_e, N, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr dice_a\n",
    "\n",
    "# delta is the threshold we used to decide whether to keep the alignment pair\n",
    "delta = 0.08\n",
    "\n",
    "for f, e in bitext:\n",
    "    posterior_e2f = defaultdict(float)\n",
    "    posterior_f2e = defaultdict(float)\n",
    "    for i, f_i in enumerate(f):\n",
    "        Z = 0\n",
    "        for j, e_j in enumerate(e):\n",
    "            Z += theta_e2f[(f_i, e_j)]\n",
    "        for j, e_j in enumerate(e):\n",
    "            posterior_e2f[(i,j)] = theta_e2f[(f_i, e_j)] / Z\n",
    "    \n",
    "    for j, e_j in enumerate(e):\n",
    "        Z = 0\n",
    "        for i, f_i in enumerate(f):\n",
    "            Z += theta_f2e[(e_j, f_i)]\n",
    "        for i, f_i in enumerate(f):\n",
    "            posterior_f2e[(j,i)] = theta_f2e[(e_j, f_i)] / Z\n",
    "\n",
    "    for pair in posterior_e2f.keys():\n",
    "        posterior = posterior_e2f[pair] * posterior_f2e[(pair[1],pair[0])]\n",
    "        if posterior > delta:\n",
    "            sys.stdout.write(f\"{pair[0]}-{pair[1]} \")\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the output to the local file dice.a\n",
    "with open('dice.a','w',encoding='utf8') as fh:\n",
    "    fh.write(str(dice_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.723117\n",
      "Recall = 0.841753\n",
      "AER = 0.230146\n"
     ]
    }
   ],
   "source": [
    "%run score-alignments.py -n 0 -i dice.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
