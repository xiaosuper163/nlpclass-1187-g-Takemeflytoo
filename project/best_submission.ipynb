{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "**Word2Vec**  \nThe main idea behind it is that you train a model on the context on each word, so similar words will have similar numerical representations.  \n**GLOVE**  \nGLOVE works similarly as Word2Vec. While you can see above that Word2Vec is a \"predictive\" model that predicts context given word, GLOVE learns by constructing a co-occurrence matrix (words X context) that basically count how frequently a word appears in a context. Since it's going to be a gigantic matrix, we factorize this matrix to achieve a lower-dimension representation.   \n**FastText**  \nFastText is quite different from the above 2 embeddings. While Word2Vec and GLOVE treats each word as the smallest unit to train on, FastText uses n-gram characters as the smallest unit. For example, the word vector ,\"apple\", could be broken down into separate word vectors units as \"ap\",\"app\",\"ple\". The biggest benefit of using FastText is that it generate better word embeddings for rare words, or even words not seen during training because the n-gram character vectors are shared with other words.   \n\n- https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n- https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge\n- https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "195649591da8c39134c885ba2f73e7e85a8812aa"
      },
      "cell_type": "code",
      "source": "import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, Conv2D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, GlobalMaxPooling1D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, concatenate, BatchNormalization\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import backend as K\nfrom keras.callbacks import *\n\nfrom gensim.models import KeyedVectors\nimport gc\nimport nltk\nfrom nltk import word_tokenize\nimport string\nimport pickle\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nimport re\ntqdm.pandas()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38c9802a86ee52c7aec85ab47e3e5d538596107a"
      },
      "cell_type": "code",
      "source": "## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 72 # max number of words in a question to use\nnum_ext_features = 12 # how many engineered features to use",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90dd47bb1cfbb1bbcd33e6974e1d1aacf7efe27b"
      },
      "cell_type": "code",
      "source": "# read the data\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "079a00140e13b94c4c169b897c45827397fef656"
      },
      "cell_type": "markdown",
      "source": "**Add new features**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90dd47bb1cfbb1bbcd33e6974e1d1aacf7efe27b"
      },
      "cell_type": "code",
      "source": "# count the number of question marks\ndef count_question_mark(x):\n    count = 0\n    for character in x:\n        if character == '?':\n            count += 1\n    return count\n\n# count the number of words\ndef count_token(x): return len(x.split())\n\n# count the number of question leading words\ndef count_wh_word(x):\n    wh_words = ['Who', 'Whose', 'What', 'Which', 'Why', 'When', 'How', 'Whose', 'Whom',\n                'Who\\'s', 'What\\'s', 'Can', 'Do', 'Does', 'Should', 'Would', 'Could', 'Will']\n    count = 0\n    for token in x.split():\n        if token in wh_words:\n            count += 1\n    return count\n\n# count the number of unique words\ndef count_unique_word(x): return len(set(str(x).split()))    \n\n# count the number of characters\ndef count_character(x): return len(str(x))\n\nstopwords = list(STOP_WORDS)\n# count the number of stopwords\ndef count_stop_word(x):\n    count = 0\n    for token in x.lower().split():\n        if token in stopwords:\n            count += 1\n    return count\n\n# count the number of punctuations\ndef count_punc(x): return len([c for c in str(x) if c in string.punctuation])\n\n# average length of the word\ndef count_avg_length(x): return np.mean([len(w) for w in str(x).split()])\n\n# count the number of top 200 insincere_words (unigram)\ntop_200_insincere_words = ['liberals', 'jews', 'racist', 'christians', 'hindus', 'democrats', 'stupid', 'hillary', 'realize',\n                           'rape', 'atheists', 'supporters', 'clinton', 'conservatives', 'liberal', 'jewish', 'immigrants', \n                           'europeans', 'republicans', 'majority', 'they', 'penis', 'guns', 'killing', 'feminists', 'blacks', \n                           'white', 'asians', 'terrorists', 'ugly', 'pakistani', 'terrorist', 'racism', 'fuck', 'western', \n                           'since', 'shit', 'black', 'palestinians', 'evil', 'dumb', 'islamic', 'transgender', \n                           'conservative', 'arabs', 'freedom', 'obsessed', 'superior', 'pakistanis', 'destroy', 'blame', \n                           'put', 'whites', 'lie', 'russians', 'males', 'crimes', 'ignorant', 'god', 'ban', 'sexually', \n                           'congress', 'africans', 'hell', 'terrorism', 'violence', 'raped', 'innocent', 'atheist', \n                           'shooting', 'admit', 'lies', 'simply', 'rude', 'worse', 'dick', 'christianity', 'quo', 'lgbt', \n                           'republican', 'western', 'females', 'party', 'democrats', 'himself', 'suck', 'lack', 'immigration',\n                           'calling', 'politicians', 'murder', 'now', 'turks', 'peace', 'deny', 'propaganda', 'israeli', \n                           'arab', 'violent', 'castrated', 'gandhi', 'refugees', 'attracted', 'homosexuality', 'trump', \n                           'generally', 'leaders', 'abuse', 'educated', 'proud', 'kashmir', 'mostly', 'syria', 'tend', \n                           'genocide', 'nazi', 'feminism', 'nations', 'flat', 'slaves', 'hatred', 'britain', 'middle', 'angry',\n                           'liberals', 'germans', 'responsible', 'abortion', 'fair', 'caste', 'commit', 'minorities', 'slavery',\n                           'people', 'jealous', 'democratic', 'dislike', 'incest', 'supporting', 'religions', 'ian', 'feminist',\n                           'turkey', 'aware', 'fbi', 'kim', 'ashamed', 'deserve', 'holocaust', 'mueller', 'minority', 'refuse', \n                           'finally', 'victims', 'crazy', 'ndra', 'muhammad', 'gays', 'complain', 'soldiers', 'blind', 'nazis',\n                           'lazy', 'democratic', 'democrat', 'proof', 'lying', 'ethnic', 'jew', 'corrupt', 'brahmins', 'ignore', \n                           'bill', 'anti', 'defend', 'cousin', 'babies', 'south', 'millions', 'wives', 'homosexual', 'allah',\n                           'privilege', 'wearing', 'mentally', 'canadians', 'banned', 'arrogant', 'barack', 'turkish', 'attacks',\n                           'races', 'beat', 'fucking', 'shootings', 'voters', 'committed', 'voted']\n\n# count the number of top 20 insincere_words (unigram)\ntop_20_insincere_words = top_200_insincere_words[:20]\ndef count_insincere_word(x):\n    count = 0\n    for token in x.lower().split():\n        if token in top_20_insincere_words:\n            count += 1\n    return count\n\n# count the number of top 100 insincere_words (unigram)\ntop_100_insincere_words = top_200_insincere_words[:100]\ndef count_100_insincere_word(x):\n    count = 0\n    for token in x.lower().split():\n        if token in top_100_insincere_words:\n            count += 1\n    return count\n\n# count the number of top 200 insincere_words (unigram)\ndef count_200_insincere_word(x):\n    count = 0\n    for token in x.lower().split():\n        if token in top_200_insincere_words:\n            count += 1\n    return count\n\n# count the number of another 100 insincere_words (unigram)\nanother_100_insincere_words = ['castrated','muslims','democrats','liberals','castrate','indians','trump','americans','women',\n                               'blacks','jews','feminists','atheists','castration','obama','homosexuals','hillary','rape',\n                               'hindus','fuck','shit','idiots','muslim','girls','christians','gay','whites','holocaust',\n                               'asians','stupid','tamils','ass','gays','jew','chinese','incest','leftists','black','crap',\n                               'men','homosexuality','white','conservatives','idiot','brahmins','modi','republicans','tamilians',\n                               'bullshit','moron','losers','terrorists','raping','fucking','moderators','shithole','dick',\n                               'palestinians','pakistanis','bhakts','europeans','liberal','jewish','penis','homosexual',\n                               'turks','nonsense','africans','tennessee','asshole','hypocrisy','mexicans','cousin','assholes',\n                               'israelis','realize','clinton','canadians','bengalis','indian','israel','uneducated','brits',\n                               'alabamians','transgender','morons','bitch','leftist','sister','aunty','democrat','supporters',\n                               'females','cock','castrating','folks','pussy','terrorist','racist','genocide']\ndef count_another_100_insincere_word(x):\n    count = 0\n    for token in x.lower().split():\n        if token in another_100_insincere_words:\n            count += 1\n    return count",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90dd47bb1cfbb1bbcd33e6974e1d1aacf7efe27b",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "new_cols = ['num_question_mark', 'num_token', 'num_wh_word', 'num_unique_word', 'num_character',\n            'num_stopword', 'num_punc', 'num_avg_length', 'num_20_insincere_word', 'num_100_insincere_word', \n            'num_200_insincere_word', 'num_another_100_insincere_word']\nnew_cols_func = [count_question_mark, count_token, count_wh_word, count_unique_word, count_character,\n                 count_stop_word, count_punc, count_avg_length, count_insincere_word, count_100_insincere_word,\n                 count_200_insincere_word, count_another_100_insincere_word]\n\n# new_cols = ['num_question_mark', 'num_token', 'num_wh_word', 'num_insincere_words']\n# new_cols_func = [count_question_mark, count_token, count_wh_word, count_insincere_word]\n\nfor i, new_col in enumerate(new_cols):\n    train_df[new_col] = train_df['question_text'].progress_apply(new_cols_func[i])\n    train_df[new_col] = (train_df[new_col]-train_df[new_col].mean())/train_df[new_col].std()\n    test_df[new_col] = test_df['question_text'].progress_apply(new_cols_func[i])\n    test_df[new_col] = (test_df[new_col]-test_df[new_col].mean())/test_df[new_col].std()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23904388f8602a52131f99b34f945f0c6c7ad181"
      },
      "cell_type": "code",
      "source": "train_df[train_df.target==1].head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d47de58e5a0c70b3cc475c4f6e4655aede201f3"
      },
      "cell_type": "code",
      "source": "min_num_200_insincere_word = train_df.num_200_insincere_word.min()\nlen(train_df[(train_df.target==1)&(train_df.num_200_insincere_word>min_num_200_insincere_word)]) / len(train_df[(train_df.target==1)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f246af836900bc5c30d1231ed900d29aabec9214"
      },
      "cell_type": "code",
      "source": "min_num_100_insincere_word = train_df.num_100_insincere_word.min()\nlen(train_df[(train_df.target==1)&(train_df.num_100_insincere_word>min_num_100_insincere_word)]) / len(train_df[(train_df.target==1)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "046d5a0d33d044775f058a975ba70f094c74d6a7"
      },
      "cell_type": "code",
      "source": "min_num_20_insincere_word = train_df.num_20_insincere_word.min()\nlen(train_df[(train_df.target==1)&(train_df.num_20_insincere_word>min_num_20_insincere_word)]) / len(train_df[(train_df.target==1)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61e8e629c8749006e167defa25d578631e2919ba"
      },
      "cell_type": "markdown",
      "source": "**Applying Latent Dirichlet Allocation(LDA) models**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec3614764d3489b402c1e07ad146f453223ed864"
      },
      "cell_type": "code",
      "source": "# punctuations = string.punctuation\n# stopwords = list(STOP_WORDS)\n# parser = English()\n# def spacy_tokenizer(sentence):\n#     mytokens = parser(sentence)\n#     mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n#     mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n#     mytokens = \" \".join([i for i in mytokens])\n#     return mytokens\n# train_df['topictext'] = train_df[\"question_text\"].progress_apply(spacy_tokenizer)\n# test_df['topictext'] = test_df[\"question_text\"].progress_apply(spacy_tokenizer)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64ed98f96b687160feea0f9972e4083f4b376ee0"
      },
      "cell_type": "code",
      "source": "# %%time\n# vectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n# vectorized = vectorizer.fit_transform(train_df['topictext'])\n\n# # Latent Dirichlet Allocation Model\n# lda_model = LatentDirichletAllocation(n_components=20, max_iter=5, learning_method='online',verbose=True)\n# lda_model.fit(vectorized)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3aacf1c5ac49d0b8127d9c0d4a16179370b98668"
      },
      "cell_type": "markdown",
      "source": "**Preprocessing when using embeddings**\n- Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings\n- Get your vocabulary as close to the embeddings as possible\n\n- https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings/"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90dd47bb1cfbb1bbcd33e6974e1d1aacf7efe27b"
      },
      "cell_type": "code",
      "source": "# glove text preprocessing\ndef clean_contractions(text):\n    mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef clean_special_chars(text):\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    return text\n\ndef correct_spelling(x):\n    dic = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\n# glove text preprocessing\ntrain_df['glove_text'] = train_df['question_text'].progress_apply(lambda x: x.lower())\ntrain_df['glove_text'] = train_df['glove_text'].progress_apply(lambda x: clean_contractions(x))\ntrain_df['glove_text'] = train_df['glove_text'].progress_apply(lambda x: clean_special_chars(x))\ntrain_df['glove_text'] = train_df['glove_text'].progress_apply(lambda x: correct_spelling(x))\n\ntest_df['glove_text'] = test_df['question_text'].progress_apply(lambda x: x.lower())\ntest_df['glove_text'] = test_df['glove_text'].progress_apply(lambda x: clean_contractions(x))\ntest_df['glove_text'] = test_df['glove_text'].progress_apply(lambda x: clean_special_chars(x))\ntest_df['glove_text'] = test_df['glove_text'].progress_apply(lambda x: correct_spelling(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22978bd95f0404ce294659bc9b3202d7b2f68d84"
      },
      "cell_type": "code",
      "source": "## fill up the missing values\ntrain_X = train_df['glove_text'].fillna(\"_na_\").values\ntest_X = test_df['glove_text'].fillna(\"_na_\").values\n## tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## get the target values\ntrain_y = train_df['target'].values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "757ab44625267b2156fccf2100eeb3e4a08ecd73"
      },
      "cell_type": "code",
      "source": "## get the engineered features\ntrain_X_ext = train_df[new_cols].values\ntest_X_ext = test_df[new_cols].values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c30c860dc14e42b5e89433dd26bcfc0035cf2fe"
      },
      "cell_type": "code",
      "source": "# # get the topics\n# train_topic = lda_model.transform(vectorizer.transform(train_df['topictext']))\n# val_topic = lda_model.transform(vectorizer.transform(val_df['topictext']))\n# test_topic = lda_model.transform(vectorizer.transform(test_df['topictext']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "757ab44625267b2156fccf2100eeb3e4a08ecd73"
      },
      "cell_type": "code",
      "source": "# # concatenate the topic features with engineered features\n# train_X_ext = np.hstack((train_X_ext, train_topic))\n# val_X_ext = np.hstack((val_X_ext, val_topic))\n# test_X_ext = np.hstack((test_X_ext, test_topic))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "44bf28cef46316380dc6750ab87d93f72973dcc0"
      },
      "cell_type": "markdown",
      "source": "**Embedding**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b17c5bf05922b7ca9577fe5ab46dc62ff7b4f6d7"
      },
      "cell_type": "code",
      "source": "def load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "73a4003ea279bfbeb2ab588588825a7ff55065db"
      },
      "cell_type": "markdown",
      "source": "* Take average of embeddings (Unweighted DME) instead of blending predictions: https://arxiv.org/pdf/1804.07983.pdf\n* The original paper of this idea comes from: Frustratingly Easy Meta-Embedding – Computing Meta-Embeddings by Averaging Source Word Embeddings"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbdc9989daea05de80294699130fcb14f922cc9b"
      },
      "cell_type": "code",
      "source": "embedding_matrix = np.mean([load_glove(tokenizer.word_index),load_para(tokenizer.word_index)], axis = 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21ca6f5211a41364f4893f7ae20742b6a7558d45"
      },
      "cell_type": "markdown",
      "source": "**Save the preprocessed dataset onto local disk to reduce redundant work**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93cd4334cbf3ad0b80a399483f6381adb9596c15"
      },
      "cell_type": "code",
      "source": "# def save_obj(x, filename):\n#     with open(filename, 'wb') as handle:\n#         pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)\n# def load_obj(filename):\n#     with open(filename, 'rb') as handle:\n#         return pickle.load(handle)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93cd4334cbf3ad0b80a399483f6381adb9596c15"
      },
      "cell_type": "code",
      "source": "# save_obj(train_X, 'train_X.pkl')\n# save_obj(train_X_ext, 'train_X_ext.pkl')\n# save_obj(train_y, 'train_y.pkl')\n# save_obj(val_X, 'val_X.pkl')\n# save_obj(val_X_ext, 'val_X_ext.pkl')\n# save_obj(val_y, 'val_y.pkl')\n# save_obj(test_X, 'test_X.pkl')\n# save_obj(test_X_ext, 'test_X_ext.pkl')\n# save_obj(embedding_matrix, 'embedding_matrix.pkl')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f07d21d6ad3e15875a15e56f34f090c5d765d3f4"
      },
      "cell_type": "code",
      "source": "# TEMP_PATH = '../input/baseline-tweaks/'\n\n# embedding_matrix = load_obj(TEMP_PATH+'embedding_matrix.pkl')\n# train_X = load_obj(TEMP_PATH+'train_X.pkl')\n# train_X_ext = load_obj(TEMP_PATH+'train_X_ext.pkl')\n# train_y = load_obj(TEMP_PATH+'train_y.pkl')\n# val_X = load_obj(TEMP_PATH+'val_X.pkl')\n# val_X_ext = load_obj(TEMP_PATH+'val_X_ext.pkl')\n# val_y = load_obj(TEMP_PATH+'val_y.pkl')\n# test_X = load_obj(TEMP_PATH+'test_X.pkl')\n# test_X_ext = load_obj(TEMP_PATH+'test_X_ext.pkl')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "062ff8df5039066449389b757248e17b4629b8d1"
      },
      "cell_type": "markdown",
      "source": "**Attention Layer**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21f0a366b9fb928211a24cfe93f862fe73fee977"
      },
      "cell_type": "code",
      "source": "from keras.engine.topology import Layer\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "446a7da8978b83814991ea5d6ef543392a079fe1"
      },
      "cell_type": "markdown",
      "source": "**Ensemble models, e.g. CNN textClassifier, LSTM**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23f29acc7652110ca915436d31b50b877639872b"
      },
      "cell_type": "code",
      "source": "# https://www.kaggle.com/yekenot/2dcnn-textclassifier\ndef model_cnn():\n    filter_sizes = [1,2,3,5]\n    num_filters = 36\n\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = Reshape((maxlen, embed_size, 1))(x)\n\n    maxpool_pool = []\n    for i in range(len(filter_sizes)):\n        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                                     kernel_initializer='he_normal', activation='elu')(x)\n        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n\n    z = Concatenate(axis=1)(maxpool_pool)   \n    z = Flatten()(z)\n    z = Dropout(0.1)(z)\n\n    outp = Dense(1, activation=\"sigmoid\")(z)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5bf6bf39a3c6e99aca39506cbb6c6743902e76c"
      },
      "cell_type": "code",
      "source": "def model_lstm_pl():\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    avg_pl = GlobalAveragePooling1D()(x)\n    max_pl = GlobalMaxPooling1D()(x)\n    concat = concatenate([avg_pl, max_pl])\n    x = Dense(64, activation=\"relu\")(concat)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47f63918f3b347fdcdc0c7f1e71e9f7a99f9d679"
      },
      "cell_type": "code",
      "source": "def model_lstm_hybrid_multi():\n    main_input = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(main_input)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x_ = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n    atten_1 = Attention(maxlen)(x)\n    atten_2 = Attention(maxlen)(x_)\n    avg_pl = GlobalAveragePooling1D()(x_)\n    max_pl = GlobalMaxPooling1D()(x_)\n    lstm_out = concatenate([avg_pl, max_pl])\n    lstm_out = Dense(64, activation=\"relu\")(lstm_out)\n    lstm_out = Dropout(0.1)(lstm_out)\n    auxiliary_output = Dense(1, activation=\"sigmoid\", name='auxiliary_output')(lstm_out)\n    \n    auxiliary_input = Input(shape=(num_ext_features,), name='aux_input')\n    x = concatenate([lstm_out, auxiliary_input])\n    x = Dense(64, activation='relu')(x)\n    x = Dense(64, activation='relu')(x)\n    main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n    \n    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], loss_weights=[1., 0.2])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e048390c6d758d5f5476c6b6068fadad1d5ca3a"
      },
      "cell_type": "code",
      "source": "def model_lstm_hybrid_single():\n    main_input = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(main_input)\n    x = SpatialDropout1D(0.1)(x)\n    x = Bidirectional(CuDNNLSTM(40, return_sequences=True))(x)\n    x_ = Bidirectional(CuDNNGRU(40, return_sequences=True))(x)\n    atten_1 = Attention(maxlen)(x)\n    atten_2 = Attention(maxlen)(x_)\n    avg_pl = GlobalAveragePooling1D()(x_)\n    max_pl = GlobalMaxPooling1D()(x_)\n    lstm_out = concatenate([atten_1, atten_2, avg_pl, max_pl])\n    lstm_out = Dense(20, activation=\"relu\")(lstm_out)\n#     lstm_out = Dropout(0.1)(lstm_out)\n    \n    auxiliary_input = Input(shape=(num_ext_features,), name='aux_input')\n    x = concatenate([lstm_out, auxiliary_input])\n    x = BatchNormalization()(x)\n    x = Dense(40, activation='relu')(x)\n    x = Dense(40, activation='relu')(x)\n    x = BatchNormalization()(x)\n    main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n    \n    model = Model(inputs=[main_input, auxiliary_input], outputs=main_output)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23bcbfdf82d0896127de0780a40879eec4101368",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# embedding_matrix = np.zeros((max_features, 300))\n# model_lstm_hybrid_single().summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c5354c0d805852472764319dc584859c503f36d"
      },
      "cell_type": "code",
      "source": "def train_pred(model, epochs=2):    \n    for e in range(epochs):\n        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n\n        best_thresh = 0.5\n        best_score = 0.0\n        for thresh in np.arange(0.1, 0.501, 0.01):\n            thresh = np.round(thresh, 2)\n            score = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n            if score > best_score:\n                best_thresh = thresh\n                best_score = score\n\n        print(\"Val F1 Score: {:.4f}\".format(best_score))\n\n    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n    return pred_val_y, pred_test_y, best_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "724f681470c7efe6837bc8862b1c3cb08ac74cca"
      },
      "cell_type": "code",
      "source": "def train_pred_hybrid_multi(model, epochs=2, callbacks=None):\n    for e in range(epochs):\n        model.fit([train_X, train_X_ext],\n                  [train_y, train_y],\n                  batch_size=512,\n                  epochs=1,\n                  validation_data=[[val_X,val_X_ext], [val_y, val_y]],\n                  callbacks = callbacks)\n        \n        pred_val_y_aux = model.predict([val_X,val_X_ext], batch_size=1024, verbose=0)[1]\n        best_thresh = 0.5\n        best_score_aux = 0.0\n        for thresh in np.arange(0.1, 1.001, 0.01):\n            thresh = np.round(thresh, 2)\n            score = metrics.f1_score(val_y, (pred_val_y_aux > thresh).astype(int))\n            if score > best_score_aux:\n                best_thresh = thresh\n                best_score_aux = score\n        print(\"Aux output, Val F1 Score: {:.4f}\".format(best_score_aux), f'Threshold: {best_thresh}')\n        \n        pred_val_y = model.predict([val_X,val_X_ext], batch_size=1024, verbose=0)[0]\n        best_thresh = 0.5\n        best_score = 0.0\n        for thresh in np.arange(0.1, 1.001, 0.01):\n            thresh = np.round(thresh, 2)\n            score = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n            if score > best_score:\n                best_thresh = thresh\n                best_score = score\n                \n        print(\"Mainoutput, Val F1 Score: {:.4f}\".format(best_score), f'Threshold: {best_thresh}')\n    \n    if best_score_aux > best_score:\n        pred_test_y = model.predict([test_X, test_X_ext], batch_size=1024, verbose=0)[1]\n    else:\n        pred_test_y = model.predict([test_X, test_X_ext], batch_size=1024, verbose=0)[0]\n    return pred_val_y, pred_test_y, best_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c2c0bc302cb8d4c4822dbba6303a3a122de8ac4"
      },
      "cell_type": "code",
      "source": "def train_pred_hybrid_single(model, epochs=2):\n    for e in range(epochs):\n        model.fit([train_X, train_X_ext],\n                  train_y,\n                  batch_size=512,\n                  epochs=1,\n                  validation_data=[[val_X,val_X_ext], val_y])\n        pred_val_y = model.predict([val_X,val_X_ext], batch_size=1024, verbose=0)\n\n        best_thresh = 0.5\n        best_score = 0.0\n        for thresh in np.arange(0.1, 1.001, 0.01):\n            thresh = np.round(thresh, 2)\n            score = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n            if score > best_score:\n                best_thresh = thresh\n                best_score = score\n\n        print(\"Mainoutput, Val F1 Score: {:.4f}\".format(best_score), f'Threshold: {best_thresh}')\n\n    pred_test_y = model.predict([test_X, test_X_ext], batch_size=1024, verbose=0)\n    return pred_val_y, pred_test_y, best_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55e4e8b7d11b38b4f5ff9ac556fbf85531e0cdef"
      },
      "cell_type": "code",
      "source": "def kfold(model, train_X, train_X_ext, train_y, val_X, val_X_ext, val_y, epochs=2, callbacks = None):\n    model.fit([train_X, train_X_ext],\n              train_y,\n              batch_size=512,\n              epochs=epochs,\n              validation_data=[[val_X,val_X_ext], val_y],\n              callbacks = callbacks,)\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "093bc8274c293e913288c8403246718bee1d08af"
      },
      "cell_type": "code",
      "source": "# outputs = []\n# pred_val_y, pred_test_y, best_score = train_pred(model_cnn(word2vec_res), word2vec_res, epochs = 2)\n# outputs.append([pred_val_y, pred_test_y, best_score, '2d CNN'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee06486a50c1ad139966a3265d8beb140cac9c57"
      },
      "cell_type": "code",
      "source": "# outputs = []\n# pred_val_y, pred_test_y, best_score = train_pred(model_lstm_atten(glove_res), glove_res, epochs = 4)\n# outputs.append([pred_val_y, pred_test_y, best_score, '2 LSTM w/ attention'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1d36477e8d62ef58665e584c7573098102388b5"
      },
      "cell_type": "code",
      "source": "# outputs = []\n# pred_val_y, pred_test_y, best_score = train_pred(model_lstm_pl(embedding_matrix), glove_res, epochs = 4)\n# outputs.append([pred_val_y, pred_test_y, best_score, '2 LSTM w/ pooling'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1abf3e9704f11f16a8c2c95b64f6cf47699c9383"
      },
      "cell_type": "code",
      "source": "# outputs = []\n# pred_val_y, pred_test_y, best_score = train_pred_hybrid_single(model_lstm_hybrid_single(embedding_matrix), epochs = 5)\n# outputs.append([pred_val_y, pred_test_y, best_score, '2 LSTM multi input single output'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5682a0982392f429d0aaaa37d13cc1a0b107d82c"
      },
      "cell_type": "code",
      "source": "DATA_SPLIT_SEED = 2018\nN_FOLD = 5\nN_EPOCH = 5\n\ntrain_meta = np.zeros(train_y.shape)\ntest_meta = np.zeros(test_X.shape[0])\nsplits = list(StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_y))\nfor idx, (train_idx, valid_idx) in enumerate(splits):\n    filepath=\"weights_best.h5\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.0001, verbose=2)\n    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n    callbacks = [checkpoint, reduce_lr]\n    # callbacks = [clr,]\n    X_train = train_X[train_idx]\n    X_train_ext = train_X_ext[train_idx]\n    y_train = train_y[train_idx]\n    X_val = train_X[valid_idx]\n    X_val_ext = train_X_ext[valid_idx]\n    y_val = train_y[valid_idx]\n    model = model_lstm_hybrid_single()\n    model = kfold(model, X_train, X_train_ext, y_train, X_val, X_val_ext, y_val, epochs = N_EPOCH, callbacks = callbacks)\n    train_meta[valid_idx] = model.predict([X_val, X_val_ext], batch_size=1024, verbose=0).reshape(-1)\n    test_meta += model.predict([test_X, test_X_ext], batch_size=1024, verbose=0).reshape(-1) / N_FOLD",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bfb4ee6e522aa209a919bcb9db2e40b46bd74d6"
      },
      "cell_type": "code",
      "source": "# https://www.kaggle.com/ryanzhang/tfidf-naivebayes-logreg-baseline\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\n\nsearch_result = threshold_search(train_y, train_meta)\nprint(search_result)\n\nsub = pd.read_csv('../input/sample_submission.csv')\nsub.prediction = (test_meta > search_result['threshold']).astype(int)\nsub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80cc7524da6cedbdd0cbd89769bc5872308b4726"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}