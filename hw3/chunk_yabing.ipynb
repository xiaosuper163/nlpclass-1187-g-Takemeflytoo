{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Phrasal Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your documentation for the chunker homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perc\n",
    "import default\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline implementation, we only care about the unigram features. The f1 score of this baseline is 91.37. Updating the weight only occurred when the predicted tag was not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_train(train_data, tagset, numepochs):\n",
    "    feat_vec = defaultdict(int)\n",
    "    default_tag = tagset[0]\n",
    "    \n",
    "    for epoch in range(numepochs):\n",
    "        \n",
    "        count_mistake = 0\n",
    "        \n",
    "        tic = time.time()\n",
    "        \n",
    "        for _, (labeled_list,feat_list) in enumerate(train_data):\n",
    "            pred_output = perc.perc_test(feat_vec, labeled_list, feat_list, tagset, default_tag)\n",
    "            true_output = [x.split()[2] for x in labeled_list]\n",
    "            \n",
    "            if pred_output != true_output:\n",
    "                count_mistake += 1\n",
    "                feat_index = 0\n",
    "                \n",
    "                for w_index in range(len(pred_output)):\n",
    "                    pred_tag = pred_output[w_index]\n",
    "                    true_tag = true_output[w_index]\n",
    "                    (feat_index, feats) = perc.feats_for_word(feat_index, feat_list)\n",
    "                    if pred_tag != true_tag:\n",
    "                        for feat in feats:\n",
    "                            feat_vec[feat, true_tag] += 1\n",
    "                            feat_vec[feat, pred_tag] -= 1\n",
    "        toc = time.time()\n",
    "        print(f'Epoch {epoch+1} finished. Time cost on this epoch: {toc-tic}. Number of mistakes: {count_mistake}.')\n",
    "    \n",
    "    return feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data ...\n",
      "done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Time cost on this epoch: 59.94477319717407. Number of mistakes: 6185.\n",
      "Epoch 2 finished. Time cost on this epoch: 64.61289858818054. Number of mistakes: 4792.\n",
      "Epoch 3 finished. Time cost on this epoch: 64.2726526260376. Number of mistakes: 3856.\n",
      "Epoch 4 finished. Time cost on this epoch: 65.9210786819458. Number of mistakes: 3104.\n",
      "Epoch 5 finished. Time cost on this epoch: 63.01786255836487. Number of mistakes: 2538.\n",
      "Epoch 6 finished. Time cost on this epoch: 63.37905955314636. Number of mistakes: 2196.\n",
      "Epoch 7 finished. Time cost on this epoch: 63.6257848739624. Number of mistakes: 1779.\n",
      "Epoch 8 finished. Time cost on this epoch: 64.33813691139221. Number of mistakes: 1542.\n",
      "Epoch 9 finished. Time cost on this epoch: 64.12444615364075. Number of mistakes: 1411.\n",
      "Epoch 10 finished. Time cost on this epoch: 66.15416932106018. Number of mistakes: 1181.\n",
      "wrote model to disk\n"
     ]
    }
   ],
   "source": [
    "feat_vec = {}\n",
    "tagset = []\n",
    "train_data = []\n",
    "tagset = perc.read_tagset(\"data/tagset.txt\")\n",
    "print(\"reading data ...\", file=sys.stderr)\n",
    "train_data = perc.read_labeled_data(\"data/train.txt.gz\", \"data/train.feats.gz\", verbose=False)\n",
    "print(\"done.\", file=sys.stderr)\n",
    "feat_vec = perc_train(train_data, tagset, 10)\n",
    "perc.perc_write_to_file(feat_vec, \"baseline.model\")\n",
    "print(\"wrote model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading test data ...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr output\n",
    "print(\"reading test data ...\", file=sys.stderr)\n",
    "test_data = perc.read_labeled_data(\"data/dev.txt\", \"data/dev.feats\", verbose=False)\n",
    "print(\"done.\", file=sys.stderr)\n",
    "feat_vec = perc.perc_read_from_file(\"baseline.model\")\n",
    "perc.perc_testall(feat_vec, test_data, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500 sentences with 10375 tokens and 5783 phrases; found phrases: 5886; correct phrases: 5331\n",
      "             ADJP: precision:  58.33%; recall:  70.71%; F1:  63.93; found:    120; correct:     99\n",
      "             ADVP: precision:  70.14%; recall:  76.73%; F1:  73.29; found:    221; correct:    202\n",
      "            CONJP: precision:  66.67%; recall:  40.00%; F1:  50.00; found:      3; correct:      5\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; F1:   0.00; found:      0; correct:      1\n",
      "               NP: precision:  90.70%; recall:  92.80%; F1:  91.73; found:   3096; correct:   3026\n",
      "               PP: precision:  96.69%; recall:  97.95%; F1:  97.31; found:   1237; correct:   1221\n",
      "              PRT: precision:  76.92%; recall:  45.45%; F1:  57.14; found:     13; correct:     22\n",
      "             SBAR: precision:  83.17%; recall:  78.50%; F1:  80.77; found:    101; correct:    107\n",
      "               VP: precision:  91.87%; recall:  91.45%; F1:  91.66; found:   1095; correct:   1100\n",
      "accuracy:  94.70%; precision:  90.57%; recall:  92.18%; F1:  91.37\n",
      "Score: 91.37\n"
     ]
    }
   ],
   "source": [
    "import score_chunks\n",
    "boundary = \"-X-\" # something to use as boundary between sentences\n",
    "outside = \"O\" # tag used to mark the outside of any chunk\n",
    "conlleval = False # use conlleval (should be False for most use cases)\n",
    "numfeats = 2 # number of columns to consider as features, typically \"word POStag\"\n",
    "(test, _) = score_chunks.readTestFile(str(output), boundary, outside, conlleval, numfeats)\n",
    "with open(\"data/reference500.txt\") as f:\n",
    "    (reference, _) = score_chunks.readTestFile(f.read(), boundary, outside, conlleval, numfeats)\n",
    "print(\"Score: %.2f\" % score_chunks.corpus_fmeasure(reference, test, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the baseline implementation, we further include the bigram features. The f1 score was improved from 91.37 to 92.36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_bigram_train(train_data, tagset, numepochs):\n",
    "    feat_vec = defaultdict(int)\n",
    "    default_tag = tagset[0]\n",
    "    \n",
    "    for epoch in range(numepochs):\n",
    "        \n",
    "        count_mistake = 0\n",
    "        \n",
    "        tic = time.time()\n",
    "        \n",
    "        for _, (labeled_list,feat_list) in enumerate(train_data):\n",
    "            pred_output = perc.perc_test(feat_vec, labeled_list, feat_list, tagset, default_tag)\n",
    "            true_output = [x.split()[2] for x in labeled_list]\n",
    "            \n",
    "            if pred_output != true_output:\n",
    "                count_mistake += 1\n",
    "                feat_index = 0\n",
    "                \n",
    "                for w_index in range(len(pred_output)):\n",
    "                    pred_tag = pred_output[w_index]\n",
    "                    true_tag = true_output[w_index]\n",
    "                    (feat_index, feats) = perc.feats_for_word(feat_index, feat_list)\n",
    "                    for feat in feats:\n",
    "                        if feat == 'B' and w_index > 0:\n",
    "                            if true_output[w_index-1] != pred_output[w_index-1] or pred_tag != true_tag:\n",
    "                                feat_vec['B:' + true_output[w_index-1], true_tag] += 1\n",
    "                                feat_vec['B:' + pred_output[w_index-1], pred_tag] -= 1\n",
    "                        elif pred_tag != true_tag:\n",
    "                            feat_vec[feat, true_tag] += 1\n",
    "                            feat_vec[feat, pred_tag] -= 1\n",
    "        toc = time.time()\n",
    "        print(f'Epoch {epoch+1} finished. Time cost on this epoch: {toc-tic}. Number of mistakes: {count_mistake}.')\n",
    "    \n",
    "    return feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data ...\n",
      "done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Time cost on this epoch: 68.16795706748962. Number of mistakes: 5594.\n",
      "Epoch 2 finished. Time cost on this epoch: 72.09285426139832. Number of mistakes: 3990.\n",
      "Epoch 3 finished. Time cost on this epoch: 74.29592108726501. Number of mistakes: 2928.\n",
      "Epoch 4 finished. Time cost on this epoch: 74.93708634376526. Number of mistakes: 2269.\n",
      "Epoch 5 finished. Time cost on this epoch: 76.31132340431213. Number of mistakes: 1849.\n",
      "Epoch 6 finished. Time cost on this epoch: 75.99922156333923. Number of mistakes: 1450.\n",
      "Epoch 7 finished. Time cost on this epoch: 81.31524085998535. Number of mistakes: 1134.\n",
      "Epoch 8 finished. Time cost on this epoch: 79.69651818275452. Number of mistakes: 946.\n",
      "Epoch 9 finished. Time cost on this epoch: 81.77506041526794. Number of mistakes: 804.\n",
      "Epoch 10 finished. Time cost on this epoch: 81.91603827476501. Number of mistakes: 739.\n",
      "wrote model to disk\n"
     ]
    }
   ],
   "source": [
    "feat_vec = {}\n",
    "tagset = []\n",
    "train_data = []\n",
    "tagset = perc.read_tagset(\"data/tagset.txt\")\n",
    "print(\"reading data ...\", file=sys.stderr)\n",
    "train_data = perc.read_labeled_data(\"data/train.txt.gz\", \"data/train.feats.gz\", verbose=False)\n",
    "print(\"done.\", file=sys.stderr)\n",
    "feat_vec = perc_bigram_train(train_data, tagset, 10)\n",
    "perc.perc_write_to_file(feat_vec, \"baseline_bigram.model\")\n",
    "print(\"wrote model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading test data ...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr output\n",
    "print(\"reading test data ...\", file=sys.stderr)\n",
    "test_data = perc.read_labeled_data(\"data/dev.txt\", \"data/dev.feats\", verbose=False)\n",
    "print(\"done.\", file=sys.stderr)\n",
    "feat_vec = perc.perc_read_from_file(\"baseline_bigram.model\")\n",
    "perc.perc_testall(feat_vec, test_data, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500 sentences with 10375 tokens and 5783 phrases; found phrases: 5755; correct phrases: 5328\n",
      "             ADJP: precision:  68.69%; recall:  68.69%; F1:  68.69; found:     99; correct:     99\n",
      "             ADVP: precision:  73.91%; recall:  75.74%; F1:  74.82; found:    207; correct:    202\n",
      "            CONJP: precision:  50.00%; recall:  20.00%; F1:  28.57; found:      2; correct:      5\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; F1:   0.00; found:      0; correct:      1\n",
      "               NP: precision:  92.98%; recall:  92.73%; F1:  92.85; found:   3018; correct:   3026\n",
      "               PP: precision:  96.24%; recall:  98.44%; F1:  97.33; found:   1249; correct:   1221\n",
      "              PRT: precision:  80.00%; recall:  36.36%; F1:  50.00; found:     10; correct:     22\n",
      "             SBAR: precision:  87.13%; recall:  82.24%; F1:  84.62; found:    101; correct:    107\n",
      "               VP: precision:  93.73%; recall:  91.09%; F1:  92.39; found:   1069; correct:   1100\n",
      "accuracy:  94.80%; precision:  92.58%; recall:  92.13%; F1:  92.36\n",
      "Score: 92.36\n"
     ]
    }
   ],
   "source": [
    "import score_chunks\n",
    "boundary = \"-X-\" # something to use as boundary between sentences\n",
    "outside = \"O\" # tag used to mark the outside of any chunk\n",
    "conlleval = False # use conlleval (should be False for most use cases)\n",
    "numfeats = 2 # number of columns to consider as features, typically \"word POStag\"\n",
    "(test, _) = score_chunks.readTestFile(str(output), boundary, outside, conlleval, numfeats)\n",
    "with open(\"data/reference500.txt\") as f:\n",
    "    (reference, _) = score_chunks.readTestFile(f.read(), boundary, outside, conlleval, numfeats)\n",
    "print(\"Score: %.2f\" % score_chunks.corpus_fmeasure(reference, test, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged weight + bigram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the baseline implementation plus bigram features, we implemented the averaged weighting approach with the guidance of the pseudo code on Page 38 of reference [Sakar 2011](http://www.cs.sfu.ca/~anoop/papers/pdf/syntax-parsing-survey-2011.pdf) and idea from section 2.5 of reference [Collins 2002](http://www.aclweb.org/anthology/W/W02/W02-1001.pdf). With this modification, the f1 score was improved from 92.36 to 93.51."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_avg_train(train_data, tagset, numepochs):\n",
    "    feat_vec = defaultdict(int)\n",
    "    avg_feat_vec = defaultdict(float)\n",
    "    default_tag = tagset[0]\n",
    "\n",
    "    for epoch in range(numepochs):\n",
    "        count_mistake = 0\n",
    "        print(f\"Running on epoch {epoch+1}......\")\n",
    "        tic = time.time()\n",
    "        for _, (labeled_list, feat_list) in enumerate(train_data):\n",
    "            pred_output = perc.perc_test(feat_vec, labeled_list, feat_list, tagset, default_tag)\n",
    "            true_output = [x.split()[2] for x in labeled_list]\n",
    "\n",
    "            if pred_output != true_output:\n",
    "                count_mistake += 1\n",
    "                feat_index = 0\n",
    "                \n",
    "                for w_index in range(len(pred_output)):\n",
    "                    pred_tag = pred_output[w_index]\n",
    "                    true_tag = true_output[w_index]\n",
    "                    (feat_index, feats) = perc.feats_for_word(feat_index, feat_list)\n",
    "                    for feat in feats:\n",
    "                        if feat == 'B' and w_index > 0:\n",
    "                            if true_output[w_index-1] != pred_output[w_index-1] or pred_tag != true_tag:\n",
    "                                feat_vec['B:' + true_output[w_index-1], true_tag] += 1\n",
    "                                feat_vec['B:' + pred_output[w_index-1], pred_tag] -= 1\n",
    "                        elif pred_tag != true_tag:\n",
    "                            feat_vec[feat, true_tag] += 1\n",
    "                            feat_vec[feat, pred_tag] -= 1\n",
    "\n",
    "\n",
    "            for key in feat_vec.keys():\n",
    "                # γ = σ/(mT)\n",
    "                avg_feat_vec[key] += feat_vec[key]\n",
    "\n",
    "        toc = time.time()\n",
    "        print(f'Epoch {epoch+1} finished. Time cost on this epoch: {toc-tic}. Number of mistakes: {count_mistake}.')\n",
    "\n",
    "    for key in avg_feat_vec.keys():\n",
    "        avg_feat_vec[key] /= (numepochs * len(train_data))\n",
    "    return avg_feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data ...\n",
      "done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on epoch 0......\n",
      "Epoch 1 finished. Time cost on this epoch: 257.43480610847473. Number of mistakes: 5594.\n",
      "Running on epoch 1......\n",
      "Epoch 2 finished. Time cost on this epoch: 454.4710409641266. Number of mistakes: 3990.\n",
      "Running on epoch 2......\n",
      "Epoch 3 finished. Time cost on this epoch: 527.4040987491608. Number of mistakes: 2928.\n",
      "Running on epoch 3......\n",
      "Epoch 4 finished. Time cost on this epoch: 569.7692408561707. Number of mistakes: 2269.\n",
      "Running on epoch 4......\n",
      "Epoch 5 finished. Time cost on this epoch: 617.2872650623322. Number of mistakes: 1849.\n",
      "Running on epoch 5......\n",
      "Epoch 6 finished. Time cost on this epoch: 632.4525303840637. Number of mistakes: 1450.\n",
      "Running on epoch 6......\n",
      "Epoch 7 finished. Time cost on this epoch: 653.9197323322296. Number of mistakes: 1134.\n",
      "Running on epoch 7......\n",
      "Epoch 8 finished. Time cost on this epoch: 1148.7454252243042. Number of mistakes: 946.\n",
      "Running on epoch 8......\n",
      "Epoch 9 finished. Time cost on this epoch: 916.5279200077057. Number of mistakes: 804.\n",
      "Running on epoch 9......\n",
      "Epoch 10 finished. Time cost on this epoch: 667.9024863243103. Number of mistakes: 739.\n",
      "wrote model to disk\n"
     ]
    }
   ],
   "source": [
    "feat_vec = {}\n",
    "tagset = []\n",
    "train_data = []\n",
    "tagset = perc.read_tagset(\"data/tagset.txt\")\n",
    "print(\"reading data ...\", file=sys.stderr)\n",
    "train_data = perc.read_labeled_data(\"data/train.txt.gz\", \"data/train.feats.gz\", verbose=False)\n",
    "print(\"done.\", file=sys.stderr)\n",
    "feat_vec = perc_avg_train(train_data, tagset, 10)\n",
    "perc.perc_write_to_file(feat_vec, \"baseline_bigram_avg.model\")\n",
    "print(\"wrote model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading test data ...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr output\n",
    "print(\"reading test data ...\", file=sys.stderr)\n",
    "test_data = perc.read_labeled_data(\"data/dev.txt\", \"data/dev.feats\", verbose=False)\n",
    "print(\"done.\", file=sys.stderr)\n",
    "# baseline\n",
    "# feat_vec = perc.perc_read_from_file(\"baseline.model\")\n",
    "# average\n",
    "feat_vec = perc.perc_read_from_file(\"baseline_bigram_avg.model\")\n",
    "perc.perc_testall(feat_vec, test_data, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500 sentences with 10375 tokens and 5783 phrases; found phrases: 5801; correct phrases: 5416\n",
      "             ADJP: precision:  71.00%; recall:  71.72%; F1:  71.36; found:    100; correct:     99\n",
      "             ADVP: precision:  77.03%; recall:  79.70%; F1:  78.35; found:    209; correct:    202\n",
      "            CONJP: precision: 100.00%; recall:  60.00%; F1:  75.00; found:      3; correct:      5\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; F1:   0.00; found:      0; correct:      1\n",
      "               NP: precision:  94.42%; recall:  94.42%; F1:  94.42; found:   3026; correct:   3026\n",
      "               PP: precision:  96.77%; recall:  98.03%; F1:  97.40; found:   1237; correct:   1221\n",
      "              PRT: precision:  80.00%; recall:  72.73%; F1:  76.19; found:     20; correct:     22\n",
      "             SBAR: precision:  84.47%; recall:  81.31%; F1:  82.86; found:    103; correct:    107\n",
      "               VP: precision:  92.84%; recall:  93.09%; F1:  92.96; found:   1103; correct:   1100\n",
      "accuracy:  95.52%; precision:  93.36%; recall:  93.65%; F1:  93.51\n",
      "Score: 93.51\n"
     ]
    }
   ],
   "source": [
    "import score_chunks\n",
    "boundary = \"-X-\" # something to use as boundary between sentences\n",
    "outside = \"O\" # tag used to mark the outside of any chunk\n",
    "conlleval = False # use conlleval (should be False for most use cases)\n",
    "numfeats = 2 # number of columns to consider as features, typically \"word POStag\"\n",
    "(test, _) = score_chunks.readTestFile(str(output), boundary, outside, conlleval, numfeats)\n",
    "with open(\"data/reference500.txt\") as f:\n",
    "    (reference, _) = score_chunks.readTestFile(f.read(), boundary, outside, conlleval, numfeats)\n",
    "print(\"Score: %.2f\" % score_chunks.corpus_fmeasure(reference, test, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
